---
title: "Feature Selection"
author: "DJL"
date: "02/10/2016"
output:
  html_document:
    toc: yes
    toc_float: yes
---

#Libraries
```{r load_libs, tidy=TRUE}
rm(list = ls())
dev.off()
library(plyr)
library(dplyr)
library(randomForest)
library(lubridate)
library(doMC)
library(caret)
library(reshape)
library(plotly)


```

#Libraries
```{r load_libs, tidy=TRUE}
#enviroment backup save function
backup_envir<- function(location=enviroment_backup,f_name="envir_backup") {
  save.image(file=paste(location,f_name,".RData",sep=""))
}



#heatmap plotting function
heatmap_ml<- function(model="NA",x_ax="NA",x_title="NA", y_ax="NA",y_title="NA",color_metric="NA",h_title="NA",save_output="NA") {
  model_data<-model$results
  model_data[,x_ax]<-as.factor(model_data[,x_ax])
  model_data[,y_ax]<-as.factor(model_data[,y_ax])
  model_data_metric<-model_data[,color_metric]
  print(str(model_data))
  ggplot(data = model_data, aes_string(x=x_ax, y=y_ax, fill=color_metric))+
    geom_tile(color = "white")+
    scale_fill_gradient2(low = "red", high = "blue", mid = "white", 
      midpoint = mean(model_data_metric), limit = c(min(model_data_metric),max(model_data_metric)), space = "Lab", 
      name="") +
    theme_minimal()+ 
    theme(axis.text.x = element_text(angle = 90, vjust = 1, 
      size = 10, hjust = 1))+
    coord_fixed()+
    ggtitle(h_title) + xlab(x_title) + ylab(y_title)
  ggsave(save_output)
}


#find simple model. 
simple_model_fun<- function(ml_model,tol_percentage = 1, metric_sm=metric) {
    tolerance(ml_model$results, metric = metric_sm, tol = tol_percentage, maximize = TRUE)    
}


```


#Directories
```{r Define directories}
data_dir <-"./P0_Characterise/output/"
P1_output_dir <-"./P1_Hypothesis_Free/output/"
P1_figs_dir <-"./P1_Hypothesis_Free/figs/"
enviroment_backup <- "./enviroment_backup/"
```

#Load Data
```{r Load data}

load(paste(data_dir,"Train_and_Test_data.Rdata",sep=""))
load(file=paste(P1_output_dir,"RFE_model_results_hypothesis_free_data.Rdata",sep=""))

#load backup enviroment
#load(file=paste(enviroment_backup,"envir_backup.RData",sep=""))

```

#Configure
```{r Define Cores and Seed}

#Allow 8 Cores
registerDoMC(cores = 8) # Note that there might be minor issues with reproducibility due to the cores that are used. 
#Set Seed
seed = 7
set.seed(seed)


#Define Metric to choose best model
metric <- "Accuracy"

#Names of all predictors to be used. 
opt_rfepredictors<-predictors(rfe_output)


#input data frame
expressionRFE<-training_df[,c("Phenotype",opt_rfepredictors)]

```

#Random Forrest Tuning
```{r Random Forrest Tuning}

#Random Forrest
rfcontrol <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid", sampling="down", savePredictions="all")
rftunegrid <- expand.grid(.mtry=(1:30))


#Run Parameter Search.
set.seed(seed)
RF_search <- train(Phenotype~., data=expressionRFE, method="rf", metric=metric, tuneGrid=rftunegrid, trControl=rfcontrol) 

#Summary and Plot. 
RF_search

```

#Stochastic Gradient Boosting, Parameter Search
```{r GBM Parameter Search}

# GBM
GBMcontrol <- trainControl(method="repeatedcv", number=10, repeats=3, sampling="down", savePredictions="all")


#tuning grid GBM
gbmGrid <-  expand.grid(shrinkage = 0.1,interaction.depth = ((1:8)*2)-1, n.trees = (1:20)*100, n.minobsinnode = 20)

#Set seed and run model
set.seed(seed)
gbmFit <- train(Phenotype ~ ., data = expressionRFE, method = "gbm", metric = metric, trControl = GBMcontrol, verbose = FALSE,tuneGrid = gbmGrid)
gbmFit


```


#SVM Poly, Parameter Search
```{r SVMpoly Parameter Search}


#tuning grid SVMpoly
svmControl <- trainControl(method="repeatedcv", number=10, repeats=3,search="random", sampling="down", savePredictions="all")
svmControl <- trainControl(method="repeatedcv", number=10, repeats=3, sampling="down", savePredictions="all")
svmGrid <-  expand.grid(degree = 3, scale = c(4^(-5:2)),C = c(4^(-2:5)))

#Set seed and run model
set.seed(seed)
svmPolyFit <- train(Phenotype ~ ., data = expressionRFE, method = "svmPoly", metric = metric, trControl = svmControl, verbose = FALSE, preProc = c("center", "scale"), tuneGrid = svmGrid )
svmPolyFit


#tuning grid SVMpoly second round informed by Grid search
svmGrid <-  expand.grid(degree = 3, scale = c((1:10)/1000),C = c(2^(-5:1),3,4))

#Set seed and run model
set.seed(seed)
svmPolyFit2 <- train(Phenotype ~ ., data = expressionRFE, method = "svmPoly", metric = metric, trControl = svmControl, verbose = FALSE, preProc = c("center", "scale"), tuneGrid = svmGrid )
svmPolyFit2
set.seed(seed)
svmPolyFit3 <- train(Phenotype ~ ., data = expressionRFE, method = "svmPoly", metric = metric, trControl = svmControl, verbose = FALSE, preProc = c("center", "scale"), tuneGrid = svmGrid )



```

#Plot for models
```{r plots}

################## RF  ############################  

ggplot(RF_search)+
  ggtitle("Random Forrest Grid Search (Accuracy)")
ggsave(paste(P1_figs_dir,"p1_3_RF_accuracy_.png",sep=""))

ggplot(RF_search, metric = "Kappa")+
  ggtitle("Random Forrest Grid Search (Kappa)")
ggsave(paste(P1_figs_dir,"p1_3_RF_Kappa_.png",sep=""))


################## GBM  ############################ 
#Grid search broad search. 


ggplot(gbmFit)+ggtitle("Stochastic Gradient Boosting Grid Search Results (Accuracy)")
ggsave(paste(P1_figs_dir,"p1_3_GBM_accuracy_.png",sep=""))

ggplot(gbmFit, metric = "Kappa")+ggtitle("Stochastic Gradient Boosting Grid Search Results (Kappa)")
ggsave(paste(P1_figs_dir,"p1_3_GBM_Kappa_.png",sep=""))


c_metric = "Accuracy"
svmTitle=paste("Stochastic Gradient Boosting Grid Search Results ","(",c_metric,")",sep="")
svmSave=paste(P1_figs_dir,"p1_3_GBM_",c_metric,"_heatmap.jpg",sep="")
heatmap_ml(model=gbmFit,x_ax="n.trees",x_title="Boosting Iterations", y_ax="interaction.depth", y_title="Max Tree Depth",color_metric=c_metric,h_title=svmTitle,save_output=svmSave )

c_metric = "Kappa"
svmTitle=paste("Stochastic Gradient Boosting Grid Search Results ","(",c_metric,")",sep="")
svmSave=paste(P1_figs_dir,"p1_3_GBM_",c_metric,"_heatmap.jpg",sep="")
heatmap_ml(model=gbmFit,x_ax="n.trees",x_title="Boosting Iterations", y_ax="interaction.depth", y_title="Max Tree Depth",color_metric=c_metric,h_title=svmTitle,save_output=svmSave )



################## SVM poly ############################ 
#Grid search broad search. 



c_metric = "Accuracy"
svmTitle=paste("SVM Polynomial intial Grid Search Results ","(",c_metric,")",sep="")
svmSave=paste(P1_figs_dir,"p1_3_svmPoly_",c_metric,"_screen_heatmap.jpg",sep="")
heatmap_ml(model=svmPolyFit,x_ax="scale",x_title="Scale", y_ax="C", y_title="Cost",color_metric=c_metric,h_title=svmTitle,save_output=svmSave )

c_metric = "Kappa"
svmTitle=paste("SVM Polynomial intial Grid Search Results ","(",c_metric,")",sep="")
svmSave=paste(P1_figs_dir,"p1_3_svmPoly_",c_metric,"_screen_heatmap.jpg",sep="")
heatmap_ml(model=svmPolyFit,x_ax="scale",x_title="Scale", y_ax="C", y_title="Cost",color_metric=c_metric,h_title=svmTitle,save_output=svmSave )

#Grid search closer look. 

c_metric = "Accuracy"
svmTitle=paste("SVM Polynomial Final Grid Search Results ","(",c_metric,")",sep="")
svmSave=paste(P1_figs_dir,"p1_3_svmPoly_",c_metric,"_final_heatmap.jpg",sep="")
heatmap_ml(model=svmPolyFit2,x_ax="scale",x_title="Scale", y_ax="C", y_title="Cost",color_metric=c_metric,h_title=svmTitle,save_output=svmSave )

c_metric = "Kappa"
svmTitle=paste("SVM Polynomial Final Grid Search Results ","(",c_metric,")",sep="")
svmSave=paste(P1_figs_dir,"p1_3_svmPoly_",c_metric,"_final_heatmap.jpg",sep="")
heatmap_ml(model=svmPolyFit2,x_ax="scale",x_title="Scale", y_ax="C", y_title="Cost",color_metric=c_metric,h_title=svmTitle,save_output=svmSave )

```

#Find low complexity model Here we look for a balance between complexity and accuracy. 
Default value I am using is 1% from top performance. 
Reduced complexity, is itself a complex ambigious thing, and not fully objective, especially when it comes to svmPoly. But for tree based methods I think there is at least general agreement.

```{r complexity vs accuracy}


#####   Find least complex model within 1 percent of top performance
simpleRF<-as.numeric(RF_search$results[simple_model_fun(ml_model=RF_search),][1,])
simpleGBM<-as.numeric(gbmFit$results[simple_model_fun(ml_model=gbmFit),][1,])
simplePoly<-as.numeric(svmPolyFit2$results[simple_model_fun(ml_model=svmPolyFit2),][1,])


#####   Define Grid searches. Etc. 
fcontrol <- trainControl(method="repeatedcv", number=10, repeats=3, sampling="down", savePredictions="all")


rftGrid <- expand.grid(.mtry=simpleRF[1])
gbmGrid <- expand.grid(shrinkage = simpleGBM[1],interaction.depth = simpleGBM[2],n.minobsinnode = simpleGBM[3], n.trees = simpleGBM[4])
svmGrid <-  expand.grid(degree = simplePoly[1], scale = simplePoly[2],C = simplePoly[3])



train(Phenotype ~ ., data = expressionRFE, method = "gbm", metric = metric, trControl = GBMcontrol, verbose = FALSE,tuneGrid = gbmGrid)
#####   Run models
set.seed(seed)
RF_final <- train(Phenotype~., data=expressionRFE, method="rf", metric=metric, trControl= fcontrol, verbose = FALSE, tuneGrid=rftGrid) 
set.seed(seed)
gbm_final <- train(Phenotype ~ ., data = expressionRFE, method = "gbm", metric = metric, trControl = fcontrol, verbose = FALSE, tuneGrid = gbmGrid)
set.seed(seed)
svmPoly_final <- train(Phenotype ~ ., data = expressionRFE, method = "svmPoly", metric = metric, trControl = fcontrol, verbose = FALSE, preProc = c("center", "scale"), tuneGrid = svmGrid)

varimp100fold<-varImp(RF_final)
varimp30fold
```


#Resample of top performers and top low complexity performers. 
```{r complexity vs accuracy}
High_Low_complexity_model_list<-list(RF = RF_search, RF_simple=RF_final, GBM = gbmFit, GBM_simple = gbm_final, svmPoly = svmPolyFit2, svmPoly_simple = svmPoly_final)

#Summary and Plot. 
ml_resample<-resamples(High_Low_complexity_model_list)
resamps=ml_resample

```

#Compare best models
```{r Compare}
str(resamps)
#define names of models for plot
Model_names<-c("RF_Complex","RF_Simple","GBM_Complex","GBM_Simple","svmPoly_Complex","svmPoly_Simple")
#plot comparing models
Accuracy_results<-resamps$values[,grep("*Accuracy",colnames(resamps$values))]
colnames(Accuracy_results)<-Model_names
accuracy_plot<-Accuracy_results[,row.names(as.matrix(sort(sapply(Accuracy_results,mean),decreasing = F)))]
accuracy_plot$Datasets<-c("RFE(GAP)")
accuracy_plot2<-melt(accuracy_plot, id="Datasets")
colnames(accuracy_plot2)<-c("GeneLists","Algorithm","Accuracy")

accuracy_plot2

title_final_models<-"Comparision of High and Low Complexity Models"
ggplot(accuracy_plot2,
       aes(x=Algorithm,y=Accuracy, colour = Algorithm)) +
  geom_point() + 
  geom_boxplot()+
  geom_jitter()+
  coord_flip()+
  ggtitle(title_final_models)
ggsave(paste(P1_figs_dir,"p1_3_Hi_Lo_complexity_model_comparision_.png",sep=""))


```

#Save all models for reference
```{r save}


#Save Models
save(High_Low_complexity_model_list, file=paste(P1_output_dir,"P1_3_High_Low_complexity_model_list.rdata",sep=""), compress = T)

```



#Validation
```{r Validation}


```

