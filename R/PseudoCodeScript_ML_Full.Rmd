---
title: "Machine Learning"
author: "DJL"
date: "02/10/2016"
output:
  html_document:
    toc: yes
    toc_float: yes
---

#Overview

**Aim**  
Create Machine learning models using GAP data. 


#Load Libraries
```{r load_libs, tidy=TRUE}

```

#Functions
```{r define functions, tidy=TRUE}




```


#Define Directories
```{r Define directories}

```

#Load Data
```{r Load data}




#Data
GAP

#Process.
Removal of LOC HS.



```

# Step 1: Split Data 
```{r Define Cores and Seed}


##### Variables
Number_of_Splits <- 10
Split_Percentage <- 0.70
  
##### Functions
#for data partition
GX_DF<-Gx_DF_3_var[,1:100]
set.seed(Number_of_Splits)
DataPartition_DF <- createDataPartition(GX_DF$Phenotype,times = Number_of_Splits,  p = Split_Percentage, list = F)


##### get training or testing DF 
if training
training_df <- GX_DF[training_index,]
else if testing
testing_df <- GX_DF[-training_index,]
else
print error



##### Clean Training DF. 

#Reduce Gene expression GAP to mirror Dejong. 
GAP_Fdata<-filter(GAP_Dejong_Full_Fdata,GAP_GAPDJ==TRUE)

#Remove all HS and Loc
GAP_Fdata <- GAP_Fdata %>% 
  mutate(LOC_HS_DROP=ifelse( grepl("^LOC",TargetID),"DROP",
                       ifelse( grepl("^HS\\.",TargetID), "DROP","KEEP"))) 
GAP_Fdata<-filter(GAP_Fdata,LOC_HS_DROP == "KEEP")
table(GAP_Fdata$LOC_HS_DROP)


table(GAP_Fdata$TargetID%in%colnames(Gx_dejong)) #should all be true. 

Gx_DF_1_locHS<-GX_DF_adj[,c("Phenotype",GAP_Fdata$TargetID)]
colnames(Gx_dejong)[1]<-"Phenotype"

table(colnames(Gx_DF_1_locHS)%in%colnames(Gx_dejong)) #All equal?



### Remove High correlation probes. 
#make dataset of features that are not probes for backup and later use.


cor_fun<- function(gene_exprs_DF,cor_cutoff=0.8) {
  # find highly correlated probes
  correlationMatrix <- cor(gene_exprs_DF)
  # find attributes that are highly corrected (ideally >0.75)
  findCorrelation(correlationMatrix, cutoff=cor_cutoff)

}


#ONLY KEEP PHENOTYPE
Gx_DF_1_locHS[1:10,1:10]
#find highly correlated probes set to 80 by default.
highlyCorrelated<-cor_fun(Gx_DF_1_locHS[,-c(1)],cor_cutoff=0.75)
highlyCorrelated


# remove highly correlated probes
Gx_DF_2_cor<-Gx_DF_1_locHS[,-c(highlyCorrelated+1)]##+1 is for phenotype col. Adds 1 to each number. 
dim(Gx_DF_2_cor)
dim(Gx_DF_1_locHS)


#Low Variance Probes removed
Gx_vars<-apply(Gx_DF_2_cor[,c(-1)], 2, function(x) var(x,))
Gx_vars_quant<-quantile(Gx_vars, c(.1,.2,.3,.4,.5))
Gx_vars_names<-names(which(Gx_vars > Gx_vars_quant[2]))
Gx_DF_3_var<-Gx_DF_2_cor[,c("Phenotype",Gx_vars_names)]

dim(Gx_DF_1_locHS)
dim(Gx_DF_2_cor)
dim(Gx_DF_3_var)



```

# Step 2: Feature Selection 
```{r Feature Selection}

Filter methods
Removal of  highly correlated probes. 80%
Low variation probes. 10% percent removal.
Diff Expression list 
Wrapper methods
Recursive feature elimination. 5x3


```

# Step 3: Build Models 
```{r Build Models}

Algorithms. 
I have a decent list of algorithms that I am using. This is useful for ensemble aswell. Not changing anything here.5x3 - 10x10 fold cross validation. 
Tuning.
I have the scripts for this, and am basically just expanding parameter search. 
Massivivly increasing grid search. 3-20 per parameter?
Ensembles.
Find models that are uncorrelated in predictions but accurate.
Linear model to combine maximum 3 models. 

```


# Step 4: Outputs
```{r Outputs}

Output:
Plots of Models
Performance
Tables for latex


```