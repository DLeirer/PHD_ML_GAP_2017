---
title: "Feature Selection"
author: "DJL"
date: "02/10/2016"
output:
  html_document:
    toc: yes
    toc_float: yes
---

#Libraries
```{r load_libs, tidy=TRUE}
rm(list = ls())
dev.off()
library(plyr)
library(dplyr)
library(randomForest)
library(lubridate)
library(doMC)
library(caret)
library(reshape)


```

#Functions
```{r define functions, tidy=TRUE}

#all machine learning models
ml_fun<- function(dataset) {
  # SVM Poly
  set.seed(seed)
  fit.svmPoly <- train(Phenotype~., data=dataset, method="svmPoly", metric=metric, preProc=c("center", "scale"), trControl=control, fit=FALSE)
  # SVM Linear
  set.seed(seed)
  fit.svmLinear <- train(Phenotype~., data=dataset, method="svmLinear2", metric=metric, preProc=c("center", "scale"), trControl=control, fit=FALSE)
  # Random Forest
  set.seed(seed)
  fit.rf <- train(Phenotype~., data=dataset, method="rf", metric=metric, trControl=control)
  # Stochastic Gradient Boosting (Generalized Boosted Modeling)
  set.seed(seed)
  fit.gbm <- train(Phenotype~., data=dataset, method="gbm", metric=metric, trControl=control, verbose=FALSE)
  ##Make List of models
  modellist<-list(svmPoly=fit.svmPoly, rf=fit.rf, gbm=fit.gbm)
  modellist
}




```



#Directories
```{r Define directories}
data_dir <-"./P0_Characterise/output/"
P1_output_dir <-"./P1_Hypothesis_Free/output/"
P1_figs_dir <-"./P1_Hypothesis_Free/figs/"

```

#Load Data
```{r Load data}

load(paste(data_dir,"Train_and_Test_data.Rdata",sep=""))
load(file=paste(P1_output_dir,"RFE_model_results_hypothesis_free_data.Rdata",sep=""))


```

#Configure
```{r Define Cores and Seed}

#Allow 8 Cores
registerDoMC(cores = 8) # Note that there might be minor issues with reproducibility due to the cores that are used. 
#Set Seed
seed = 7
set.seed(seed)

###### Control for algorithms #########
# Random Forrest
rfcontrol <- trainControl(method="repeatedcv", number=10, repeats=3, sampling="down", savePredictions="all")
rftunegrid <- expand.grid(.mtry=c(1:15), .ntree=c(1000, 1500, 2000, 2500))
rftunegrid <- expand.grid(.mtry=c(1:30))
# GBM
GBMcontrol <- trainControl(method="repeatedcv", number=10, repeats=3, sampling="down", savePredictions="all")


#Define Metric to choose best model
metric <- "Accuracy"

#Names of all predictors to be used. 
opt_rfepredictors<-predictors(rfe_output)


#input data frame
expressionRFE<-training_df[,c("Phenotype",opt_rfepredictors)]
#expressionRFE<-training_df[,c("Phenotype",opt_rfepredictors[1:50])]


```

#Random Forrest Tuning
```{r Random Forrest Tuning}

#Define Custom function, taken from MachineLearning Mastery. Looks at mtry and ntree at same time. 
customRF <- list(type = "Classification", library = "randomForest", loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"), class = rep("numeric", 2), label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
    predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
    predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes
customRF

#Run Parameter Search.
set.seed(seed)
RF_custom_search <- train(Phenotype~., data=expressionRFE, method=customRF, metric=metric, tuneGrid=rftunegrid, trControl=rfcontrol) 
RF_custom_search_test <- train(Phenotype~., data=expressionRFE, method="rf", metric=metric, tuneGrid=rftunegrid, trControl=rfcontrol) 
#takes around 15 minutes with 8 cores and 60 runs at 30 x cross validation. 

#Summary and Plot. 
RF_custom_search_test
str(RF_custom_search_test)
plot(RF_custom_search_test)
trellis.par.set(caretTheme())
plot(RF_custom_search_test)  
```

#Stochastic Gradient Boosting, Parameter Search
```{r GBM Parameter Search}

#tuning grid GBM
gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9), n.trees = (1:30)*50, shrinkage = 0.1, n.minobsinnode = 20)

#Set seed and run model
set.seed(seed)
gbmFit1 <- train(Phenotype ~ ., data = expressionRFE, method = "gbm",trControl = GBMcontrol, verbose = FALSE,tuneGrid = gbmGrid)
gbmFit1

trellis.par.set(caretTheme())
plot(gbmFit1)  
trellis.par.set(caretTheme())
plot(gbmFit1, metric = "Kappa")

whichTwoPct <- tolerance(gbmFit1$results, metric = metric, tol = 2, maximize = TRUE)  
gbmFit1$results[whichTwoPct,1:6]

predict(gbmFit1, newdata = head(testing))


trellis.par.set(caretTheme())
plot(gbmFit1, metric = "Kappa", plotType = "level",
     scales = list(x = list(rot = 90)))

```


#SVM Poly, Parameter Search
```{r SVMpoly Parameter Search}


```

