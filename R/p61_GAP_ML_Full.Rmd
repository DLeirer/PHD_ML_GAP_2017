---
title: "Machine Learning"
author: "DJL"
date: "02/10/2016"
output:
  html_document:
    toc: yes
    toc_float: yes
---

#Overview

**Aim**  
Create Machine learning models using GAP data. 


#Load Libraries
```{r load_libs, tidy=TRUE}
rm(list = ls())
dev.off()
library(plyr)
library(dplyr)
library(lubridate)
library(doMC)
library(caret)
library(reshape)
library(tableone)
library(stargazer)
library(mice)
library(caretEnsemble)

```

#Functions
```{r define functions, tidy=TRUE}
#function for Recursive feature elimination. Requires helper functions below. 
f_rfe_tolist <- function (GX_DF, Resample_list, subsets, control, seed){
  Resample_df<-Resample_list[["DataPartition"]]
  n_repeats<-dim(Resample_df)[2]
  #start loop
  for (i in 1:n_repeats){
    #print loop
    print(paste("Resample ",i))
    #Recursive feature elimination
    #Create Training DF
    train_df<-f_train_test(GX_DF,DataPartition_DF,i,T)
    print(train_df[1:3,1:5])
    #Create Clean DF
    train_df_small<-f_train_clean(train_df,cor_cutoff = 0.75)
    print("Final Train df Size")
    print(dim(train_df_small))
    print(train_df_small[1:3,1:5])
    #Recursive Feature Elmination.
    rfe_output<-f_rfe(train_df_small, subsets, control, seed)
    #get predictors and plot add to lists. 
    Resample_list$rfelist[[i]]<-predictors(rfe_output)
    Resample_list$plottestlist[[i]]<-plot(rfe_output, type = c("g", "o"),col="green",main="Recursive Feature Elimination")
    print("Recursive Feature Elminiation done and Added to list")
  }
  return(Resample_list)
}



#Get train or test DF function # helper function to f_rfe_tolist
f_train_test <- function (Data ,indexDF ,index ,Training = TRUE){
  training_index <- indexDF[,index]  
  if (Training == TRUE){
    print("Training dataframe created")
    training <- Data[training_index,]
    return (training) 
  }else{
    print("Testing dataframe created")
    testing <- Data[-training_index,]
    return (testing)
  }
}



# Clean Training DF function. Correlation and variance.   # helper function to f_rfe_tolist
f_train_clean <- function (train_df, cor_cutoff=0.75,low_variance_cut=0.25){
  ### Remove High correlation probes. 
  #make dataset of features that are not probes for backup and later use.
  highlyCorrelated<-cor_fun(train_df[,-c(1)],cor_cutoff=cor_cutoff)
  # remove highly correlated probes
  train_df<-train_df[,-c(highlyCorrelated+1)]##+1 is for phenotype col. Adds 1 to each number. 
  
  #Low Variance Probes removed
  Gx_vars<-apply(train_df[,c(-1)], 2, function(x) var(x,y = NULL, na.rm = FALSE, "everything"))
  Gx_vars_quant<-quantile(Gx_vars, c(low_variance_cut))
  Gx_vars_names<-names(which(Gx_vars > Gx_vars_quant[1]))
  train_df<-train_df[,c("Phenotype",Gx_vars_names)]
  return(train_df)
}


#Correlation Function # helper function to f_train_clean
cor_fun<- function(gene_exprs_DF,cor_cutoff=0.75) {
  # find highly correlated probes
  correlationMatrix <- cor(gene_exprs_DF)
  # find attributes that are highly corrected (ideally >0.75)
  findCorrelation(correlationMatrix, cutoff=cor_cutoff)
}



#Recursive feature elimination function. # helper function to f_rfe_tolist
f_rfe <- function (train_df_small, subsets, control, seed){
  ### Define Class Labels
  class_labels <- droplevels(as.factor(train_df_small[,1]))
  ## Define Predictors
  gx_predictors=train_df_small[,-1]
  #set seed
  set.seed(seed)
  # run the RFE algorithm
  rfe_output <- rfe(gx_predictors, class_labels, sizes=subsets, rfeControl=control)
  return(rfe_output)
}

###########   Functions for machine learning ############################################################

# 1. function for train ensemble df. 
f_train_ensemble <- function (GX_Data,Resample_list,index){
  #Get Train DF
  train_ensemble_df<-f_train_test(GX_Data,Resample_list_results$DataPartition,index,T)
  #subset to predictors
  train_ensemble_df<-train_ensemble_df[,c("Phenotype",Resample_list$rfelist[[index]])]
  return (train_ensemble_df)
}



# 2. function for caret list. Machine Learning.
f_caret_list_ML <- function (GX_train,boot_n){
  
  #set seed
  set.seed(seed)
  #define indices
  index=createResample(GX_train$Phenotype, boot_n)
  print(index)
  #set control
  my_control <- trainControl(method="boot", 
                             number=boot_n,
                             index=index,
                             classProbs = TRUE,
                             summaryFunction=twoClassSummary,
                             savePredictions="final",
                             preProc=c("center", "scale"),
                             sampling="down")
  
  #set seed
  set.seed(seed)
  #Machine learning Cart List. 
  return(caretList(Phenotype~., 
                    data=GX_train, 
                    trControl=my_control,
                    metric=metric,
                    tuneList=ML_list2,
                    continue_on_fail=T))
}


# 3. function for caret stack. Blend Meta models. 
#Caret stack function
f_run_caret_stack <- function (model_list,ensemble_method="gbm",ensemble_tune=10){
  #save caret stack
  return(caretStack(
    model_list,
    method=ensemble_method,
    verbose=FALSE,
    tuneLength=ensemble_tune,
    metric="ROC",
    trControl=trainControl(
      method="boot",
      number=25,
      savePredictions="final",
      classProbs=TRUE,
      summaryFunction=twoClassSummary)
    )
  )
}

# 4. get caret list and stack results
f_get_caret_stack_results <- function (caret_list_data,ensemble_data,test_df,probability=TRUE,Pheno="Control"){
  if (probability == TRUE){
    print("creating probability df")
    model_predi <- lapply(caret_list_data, predict, newdata=test_df, type="prob")
    model_predi <- lapply(model_predi, function(x) x[,Pheno])
    model_predi <- data.frame(model_predi)
    model_predi$ensemble <- predict(ensemble_data, newdata=test_df, type="prob")
    return(model_predi)
  }else{
    print("creating categorical df")
    model_predi <- lapply(caret_list_data, predict, newdata=test_df)
    model_predi <- data.frame(model_predi)
    model_predi$ensemble <- predict(ensemble_data, newdata=test_df)
    model_predi$Phenotype<-test_df[,1]
    return (model_predi)
  }
}

f_save_results_1 <- function (results_resample,model_list=model_list,test_df,Sub_prefix=Sub_prefix){
  results_resample[["gx_testing"]] <- test_df
  results_resample[["caret_list"]]<- model_list
  results_resample[["caret_list_model_preds"]] <- lapply(model_list, predict, newdata=test_df)
  results_resample[["splom"]]<-splom(resamples(model_list))
  results_resample[["modelcor"]]<-modelCor(resamples(model_list))
  #save list for top level resample (initial train test split)
  save(results_resample, file=paste(output_dir,project_id,project_name,Sub_prefix,top_split,".rdata",sep=""), compress = T)
  return(results_resample)
}


f_save_results_2 <- function (results_resample,model_list=model_list,test_df,ensemble_model,Sub_prefix){
  results_resample[["ensemble_full"]]<-ensemble_model #save ensemble
  results_resample[["ensemble_summary"]]<-summary(ensemble_model) #save ensemble categoical
  results_resample[["ensemble_model_preds_cat"]]<-f_get_caret_stack_results(model_list,ensemble_model,test_df,probability = F)
  model_preds<-f_get_caret_stack_results(model_list,ensemble_model,test_df,probability = T) 
  results_resample[["ensemble_model_preds_probability"]]<-model_preds #save model preds probability
  results_resample[["ensemble_test_performance"]]<-caTools::colAUC(model_preds, test_df$Phenotype) #save comparision
  #save results
  save(results_resample, file=paste(output_dir,project_id,project_name,Sub_prefix,top_split,".rdata",sep=""), compress = T)
  return(results_resample)
}

# 5 function drawing on 1-4
#Function creating models and blend
f_caret_list_ensemble <- function (GX_DF=GX_DF,Resample_list_results=Resample_list_results,top_split=top_split,boot_n=boot_n,fold=x){

  #get train ensemble df
  train_ensemble_df<-f_train_ensemble(GX_DF,Resample_list_results,x)

  #Machine learning
  set.seed(seed)
  model_list<-f_caret_list_ML(train_ensemble_df,boot_n)  
  print(model_list)
  print(paste("Caret_list for fold",fold,"done.",sep=" "))
  ###save caret list
  #define test data
  testing<-f_train_test(GX_DF,Resample_list_results$DataPartition,x,F)

  # Output data caret list. 
  #save initial results
  results_output<-list()
  results_output<-f_save_results_1(results_output,model_list,test_df=testing,Sub_prefix)
  print(paste("Caret_list for fold",fold,"saved.",sep=" "))
  
  #create ensemble
  set.seed(seed)  
  ensemble_final<-f_run_caret_stack(model_list)
  print(paste("Caret_stack for fold",fold,"done.",sep=" "))  

  #create results output
  results<-f_save_results_2(results_output,model_list,test_df=testing,ensemble_final,Sub_prefix)
  print(paste("Caret_stack for fold",fold,"saved.",sep=" "))  
  return(results)
}


```


#Define Directories
```{r Define directories}
top_dir<-getwd()
data_dir <-"./P0_Characterise/output/"
data_dir0 <-"./data/"
data_dejong_dir<-"./P00_Characterise_Dejong/output/"
output_dir <-"./P61_GAP_2way/output/"
figs_dir <-"./P61_GAP_2way/figs/"
```

#Load Data
```{r Load data}

#Load
load(file=paste(data_dejong_dir,"Dejong_reduced_Gx_data_and_pheno.RData",sep=""))
load(file=paste(data_dir,"GX_DF_adj_data.Rdata",sep=""))


#Reduce Gene expression GAP to mirror Dejong. 
GAP_Fdata<-filter(GAP_Dejong_Full_Fdata,GAP_GAPDJ==TRUE)

#Remove all HS and Loc
GAP_Fdata <- GAP_Fdata %>% 
  mutate(LOC_HS_DROP=ifelse( grepl("^LOC",TargetID),"DROP",
                       ifelse( grepl("^HS\\.",TargetID), "DROP","KEEP"))) 
GAP_Fdata<-filter(GAP_Fdata,LOC_HS_DROP == "KEEP")

#Create Dataframe
GX_DF<-GX_DF_adj[,c("Phenotype",GAP_Fdata$TargetID)] #Expression data with Phenotype as first column

#Centre and Scale. 
preProcValues <- preProcess(GX_DF, method = c("center", "scale"))
GX_DF <- predict(preProcValues, GX_DF)

dim(GX_DF) #3919 features

```


# Settings
```{r Define Cores and Seed}
#Allow 8 Cores
registerDoMC(cores = 8) 
#Set Seed
seed = 7
```

# Step 1: Split Data 
```{r split data}
############Step 1 Functions
##### Variables
Number_of_Splits <- 10 #Number of splits
#Number_of_Splits <- 2 #Number of splits
Split_Percentage <- 0.80 #Split of data
project_id = "p61_"
project_name = "GAP_2way"
Sub_prefix = "_Step2_"


#for data partition
set.seed(Number_of_Splits)
DataPartition_DF <- createDataPartition(GX_DF$Phenotype,times = Number_of_Splits,  p = Split_Percentage, list = F)

```

# Step 2: Feature Selection 
```{r Feature Selection}

#Variables for Step 2
#set number of subsets to test
subsets <- c(1:30)*25
#subsets <- c(1:3)*25
boot_n=25
#boot_n=5
#set control parameters 
control <- rfeControl(functions=rfFuncs, #Random Forrest based
                      method="boot", #Repeated CV
                      number=boot_n, #10 folds
                      #repeats=40, #10 repeats
                      verbose = FALSE)

#Create output list object. 
Resample_list<-list()
Resample_list[["DataPartition"]]<-DataPartition_DF
Resample_list[["rfelist"]]<-list()
Resample_list[["plottestlist"]]<-list()

#Run Feature elimination. 
Resample_list_results<-f_rfe_tolist(GX_DF, Resample_list, subsets, control, seed)

#save list
save(Resample_list_results, file=paste(output_dir,project_id,project_name,Sub_prefix,"_Resample_List",".rdata",sep=""), compress = T)


```

# Step 3: Build Models 
```{r Build Models}

Algorithms. 
I have a decent list of algorithms that I am using. This is useful for ensemble aswell. Not changing anything here.5x3 - 10x10 fold cross validation. 
Tuning.
I have the scripts for this, and am basically just expanding parameter search. 
Massivivly increasing grid search. 3-20 per parameter?
Ensembles.
Find models that are uncorrelated in predictions but accurate.
Linear model to combine maximum 3 models. 

#load resamples
load(file=paste(output_dir,project_id,project_name,Sub_prefix,"_Resample_List",".rdata",sep=""))

#cut down probes to number of samples. 
#Number_of_Splits <- 10 #Number of splits
for (Split_n in 1:Number_of_Splits){ 
    split<-Resample_list_results$rfelist[[Split_n]]
    if (length(split) > dim(Resample_list_results[[1]])[1]){
      Resample_list_results$rfelist[[Split_n]]<-split[1:dim(Resample_list_results[[1]])[1]]
    }
}

#Variables

#Define Seed
seed <- 10
#modelID<-c("lda","glm","gbm","glmnet","nb","knn","svmLinear2","svmPoly","svmRadial","rf","C5.0","parRF","elm","nnet","avNNet","pcaNNet","xgbTree")
ML_list2<-list(
    svmLinear2=caretModelSpec(method="svmLinear2", tuneLength=10),
    svmPoly=caretModelSpec(method="svmPoly", tuneLength=10),
    glmnet=caretModelSpec(method="glmnet",tuneLength=10),
    nb=caretModelSpec(method="nb",tuneLength=5),
    pcaNNet=caretModelSpec(method="pcaNNet",tuneLength=10),
    rf=caretModelSpec(method="rf", tuneGrid=expand.grid(.mtry=(1:30)))    
)

#ML_list2<-list(
#    glmnet=caretModelSpec(method="glmnet",tuneLength=3),
#    rf=caretModelSpec(method="rf", tuneGrid=expand.grid(.mtry=(1:3)))    
#)



#Define Metric to choose best model
metric = "ROC"
# 4. Meta function looping and saving everything appropriatly. 
#set bootstrap iterations
boot_n = 25
#boot_n = 3

#Run all models. 
for (i in 1:Number_of_Splits){
  #fold
  x=i
  #set top split ID
  top_split = paste("Split_",x,sep="")
  #caret list and ensemble
  f_caret_list_ensemble(GX_DF=GX_DF,Resample_list_results=Resample_list_results,top_split=top_split,boot_n=boot_n,fold=x)
}



############################################################################
#Load all models saved ensemble runs.

full_data<-list() #empty list wrapper. 
for (i in 1:Number_of_Splits){
  #fold
  x=i
  #set top split ID
  top_split = paste("Split_",x,sep="")
  print(top_split)
  load(paste(output_dir,project_id,project_name,Sub_prefix,top_split,".rdata",sep=""))
  full_data[[top_split]]<-results_resample
}




summary(full_data[[split]]$ensemble_full)
split = 10
full_data[[split]]$ensemble_test_performance
table(full_data$Split_2$gx_testing[,split])
lapply(full_data$Split_1$ensemble_model_preds_cat,table)
table(full_data$Split_1$ensemble_model_preds_cat[,8])
full_data$Split_2$ensemble_model_preds_cat

full_data[[split]]$ensemble_full





#### dejong
#clean Dejong
GX_dejong_final<-Gx_dejong %>% mutate(Phenotype=ifelse(Diagnosis == "control","Control","FEP"))
features<-dim(GX_dejong_final)[2]
GX_dejong_final<-GX_dejong_final[,c(features,1:(features-1))]
GX_dejong_final<-GX_dejong_final[,-c(2)]

#test caret list in dejong

summary(full_data[[split]]$ensemble_full)
#test caret stack in dejong
Dejong_results_ensemble<-list() #empty list wrapper. 
for (i in 1:Number_of_Splits){
  #fold
  x=i
  #set top split ID
  top_split = paste("Split_",x,sep="")
  print(top_split)
  gbm_ensemble<-full_data[[top_split]]$ensemble_full
  Dejong_results_ensemble[[top_split]]<-predict(gbm_ensemble, newdata=GX_dejong_final)
}

for (i in 1:Number_of_Splits){
  #fold
  x=i
  #set top split ID
  top_split = paste("Split_",x,sep="")
  print(top_split)
  print(confusionMatrix(Dejong_results_ensemble[[top_split]], GX_dejong_final$Phenotype))

}


Test_data_1 <- Dejong_cont_pre
Dejong_results_ensemble2<-list() #empty list wrapper. 
for (i in 1:Number_of_Splits){
  #fold
  x=i
  #set top split ID
  top_split = paste("Split_",x,sep="")
  print(top_split)
  gbm_ensemble<-full_data[[top_split]]$ensemble_full
  Dejong_results_ensemble2[[top_split]]<-predict(gbm_ensemble, newdata=Test_data_1)
  print(confusionMatrix(Dejong_results_ensemble2[[top_split]], Test_data_1$Phenotype))
}


dejong_model_preds

cbind(GX_dejong_final[1],Dejong_results_ensemble$Split_1)

Dejong_cont_pre$Phenotype
predict_fit<-predict(full_data[[1]]$ensemble_full, newdata = GX_dejong_final)
confusionMatrix(predict_fit, GX_dejong_final$Phenotype)

full_data$Split_1$caret_list_model_preds

str(caret_list_1)

library("caret")
library("mlbench")
library("pROC")
data(Sonar)
set.seed(107)
inTrain <- createDataPartition(y = Sonar$Class, p = .75, list = FALSE)
training <- Sonar[ inTrain,]
testing <- Sonar[-inTrain,]
my_control <- trainControl(
  method="boot",
  number=25,
  savePredictions="final",
  classProbs=TRUE,
  index=createResample(training$Class, 25),
  summaryFunction=twoClassSummary
  )

library("rpart")
library("caretEnsemble")
model_list <- caretList(
  Class~., data=training,
  trControl=my_control,
  methodList=c("glm", "rpart")
  )
resamples(model_list)



model_preds <- lapply(, predict, newdata=GX_dejong_final, type="prob")

```


# Step 4: Outputs
```{r Outputs}


```








