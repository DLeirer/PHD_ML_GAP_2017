---
title: "Machine Learning"
author: "DJL"
date: "02/10/2016"
output:
  html_document:
    toc: yes
    toc_float: yes
---

#Overview

**Aim**  
Create Machine learning models using GAP data. 

#Old Functions
```{r old functions, tidy=TRUE}
#function for Recursive feature elimination. Requires helper functions below. 
f_rfe_tolist <- function (GX_DF, Resample_list, subsets, control, seed){
  Resample_df<-Resample_list[["DataPartition"]]
  n_repeats<-dim(Resample_df)[2]
  #start loop
  for (i in 1:n_repeats){
    #print loop
    print(paste("Resample ",i))
    #Recursive feature elimination
    #Create Training DF
    train_df<-f_train_test(GX_DF,DataPartition_DF,i,T)
    print(train_df[1:3,1:5])
    #Create Clean DF
    train_df_small<-f_train_clean(train_df,cor_cutoff = 0.75)
    print("Final Train df Size")
    print(dim(train_df_small))
    print(train_df_small[1:3,1:5])
    #Recursive Feature Elmination.
    rfe_output<-f_rfe(train_df_small, subsets, control, seed)
    #get predictors and plot add to lists. 
    Resample_list$rfelist[[i]]<-predictors(rfe_output)
    Resample_list$plottestlist[[i]]<-plot(rfe_output, type = c("g", "o"),col="green",main="Recursive Feature Elimination")
    print("Recursive Feature Elminiation done and Added to list")
  }
  return(Resample_list)
}



#Get train or test DF function # helper function to f_rfe_tolist
f_train_test <- function (Data ,indexDF ,index ,Training = TRUE){
  training_index <- indexDF[,index]  
  if (Training == TRUE){
    print("Training dataframe created")
    training <- Data[training_index,]
    return (training) 
  }else{
    print("Testing dataframe created")
    testing <- Data[-training_index,]
    return (testing)
  }
}



# Clean Training DF function. Correlation and variance.   # helper function to f_rfe_tolist
f_train_clean <- function (train_df, cor_cutoff=0.75,low_variance_cut=0.25){
  ### Remove High correlation probes. 
  #make dataset of features that are not probes for backup and later use.
  highlyCorrelated<-cor_fun(train_df[,-c(1)],cor_cutoff=cor_cutoff)
  # remove highly correlated probes
  train_df<-train_df[,-c(highlyCorrelated+1)]##+1 is for phenotype col. Adds 1 to each number. 
  
  #Low Variance Probes removed
  Gx_vars<-apply(train_df[,c(-1)], 2, function(x) var(x,y = NULL, na.rm = FALSE, "everything"))
  Gx_vars_quant<-quantile(Gx_vars, c(low_variance_cut))
  Gx_vars_names<-names(which(Gx_vars > Gx_vars_quant[1]))
  train_df<-train_df[,c("Phenotype",Gx_vars_names)]
  return(train_df)
}


#Correlation Function # helper function to f_train_clean
cor_fun<- function(gene_exprs_DF,cor_cutoff=0.75) {
  # find highly correlated probes
  correlationMatrix <- cor(gene_exprs_DF)
  # find attributes that are highly corrected (ideally >0.75)
  findCorrelation(correlationMatrix, cutoff=cor_cutoff)
}



#Recursive feature elimination function. # helper function to f_rfe_tolist
f_rfe <- function (train_df_small, subsets, control, seed){
  ### Define Class Labels
  class_labels <- droplevels(as.factor(train_df_small[,1]))
  ## Define Predictors
  gx_predictors=train_df_small[,-1]
  #set seed
  set.seed(seed)
  # run the RFE algorithm
  rfe_output <- rfe(gx_predictors, class_labels, sizes=subsets, rfeControl=control)
  return(rfe_output)
}

###########   Functions for machine learning ############################################################

# 1. function for train ensemble df. 
f_train_ensemble <- function (GX_Data,Resample_list,index){
  #Get Train DF
  train_ensemble_df<-f_train_test(GX_Data,Resample_list_results$DataPartition,index,T)
  #subset to predictors
  train_ensemble_df<-train_ensemble_df[,c("Phenotype",Resample_list$rfelist[[index]])]
  return (train_ensemble_df)
}



# 2. function for caret list. Machine Learning.
f_caret_list_ML <- function (GX_train,boot_n){
  
  #set seed
  set.seed(seed)
  #define indices
  index=createResample(GX_train$Phenotype, boot_n)
  print(index)
  #set control
  my_control <- trainControl(method="boot", 
                             number=boot_n,
                             index=index,
                             classProbs = TRUE,
                             summaryFunction=twoClassSummary,
                             savePredictions="final",
                             preProc=c("center", "scale"),
                             sampling="down")
  
  #set seed
  set.seed(seed)
  #Machine learning Cart List. 
  return(caretList(Phenotype~., 
                    data=GX_train, 
                    trControl=my_control,
                    metric=metric,
                    tuneList=ML_list2,
                    continue_on_fail=T))
}


# 3. function for caret stack. Blend Meta models. 
#Caret stack function
f_run_caret_stack <- function (model_list,ensemble_method="gbm",ensemble_tune=10){
  #save caret stack
  return(caretStack(
    model_list,
    method=ensemble_method,
    verbose=FALSE,
    tuneLength=ensemble_tune,
    metric="ROC",
    trControl=trainControl(
      method="boot",
      number=25,
      savePredictions="final",
      classProbs=TRUE,
      summaryFunction=twoClassSummary)
    )
  )
}

# 4. get caret list and stack results

f_save_results_1 <- function (results_resample,model_list=model_list,test_df,Sub_prefix=Sub_prefix){
  results_resample[["gx_testing"]] <- test_df
  results_resample[["caret_list"]]<- model_list
  results_resample[["caret_list_model_preds"]] <- lapply(model_list, predict, newdata=test_df)
  results_resample[["splom"]]<-splom(resamples(model_list))
  results_resample[["modelcor"]]<-modelCor(resamples(model_list))
  #save list for top level resample (initial train test split)
  save(results_resample, file=paste(output_dir,project_id,project_name,Sub_prefix,top_split,".rdata",sep=""), compress = T)
  return(results_resample)
}


f_save_results_2 <- function (results_resample,model_list=model_list,test_df,ensemble_model,Sub_prefix){
  results_resample[["ensemble_full"]]<-ensemble_model #save ensemble
  results_resample[["ensemble_summary"]]<-summary(ensemble_model) #save ensemble categoical
  results_resample[["ensemble_model_preds_cat"]]<-f_get_caret_stack_results(model_list,ensemble_model,test_df,probability = F)
  model_preds<-f_get_caret_stack_results(model_list,ensemble_model,test_df,probability = T) 
  results_resample[["ensemble_model_preds_probability"]]<-model_preds #save model preds probability
  results_resample[["ensemble_test_performance"]]<-caTools::colAUC(model_preds, test_df$Phenotype) #save comparision
  #save results
  save(results_resample, file=paste(output_dir,project_id,project_name,Sub_prefix,top_split,".rdata",sep=""), compress = T)
  return(results_resample)
}

# 5 function drawing on 1-4
#Function creating models and blend
f_caret_list_ensemble <- function (GX_DF=GX_DF,Resample_list_results=Resample_list_results,top_split=top_split,boot_n=boot_n,fold=x){

  #get train ensemble df
  train_ensemble_df<-f_train_ensemble(GX_DF,Resample_list_results,x)

  #Machine learning
  set.seed(seed)
  model_list<-f_caret_list_ML(train_ensemble_df,boot_n)  
  print(model_list)
  print(paste("Caret_list for fold",fold,"done.",sep=" "))
  ###save caret list
  #define test data
  testing<-f_train_test(GX_DF,Resample_list_results$DataPartition,x,F)

  # Output data caret list. 
  #save initial results
  results_output<-list()
  results_output<-f_save_results_1(results_output,model_list,test_df=testing,Sub_prefix)
  print(paste("Caret_list for fold",fold,"saved.",sep=" "))
  
  #create ensemble
  set.seed(seed)  
  ensemble_final<-f_run_caret_stack(model_list)
  print(paste("Caret_stack for fold",fold,"done.",sep=" "))  

  #create results output
  results<-f_save_results_2(results_output,model_list,test_df=testing,ensemble_final,Sub_prefix)
  print(paste("Caret_stack for fold",fold,"saved.",sep=" "))  
  return(results)
}


```

#Load Data
```{r Load data}

#Load
load(file=paste(data_dejong_dir,"Dejong_reduced_Gx_data_and_pheno.RData",sep=""))
load(file=paste(data_dir,"GX_DF_adj_data.Rdata",sep=""))


#Reduce Gene expression Dejong to mirror GAP. 
dejong_Fdata<-filter(GAP_Dejong_Full_Fdata,DeJong_GAPDJ==TRUE)

#Remove all HS and Loc
dejong_Fdata <- dejong_Fdata %>% 
  mutate(LOC_HS_DROP=ifelse( grepl("^LOC",TargetID),"DROP",
                             ifelse( grepl("^HS\\.",TargetID), "DROP","KEEP"))) 
dejong_Fdata<-filter(dejong_Fdata,LOC_HS_DROP == "KEEP")


#clean Dejong
GX_DF<-Gx_dejong %>% mutate(Phenotype=ifelse(Diagnosis == "control","Control","FEP"))
features<-dim(GX_DF)[2]
GX_DF<-GX_DF[,c(features,1:(features-1))]
GX_DF<-GX_DF[,-c(2)]


#Create Dataframe
GX_DF<-GX_DF[,c("Phenotype",dejong_Fdata$TargetID)] #Expression data with Phenotype as first column



#Centre and Scale. 
preProcValues <- preProcess(GX_DF, method = c("center", "scale"))
GX_DF <- predict(preProcValues, GX_DF)

dim(GX_DF) #3919 features
GX_DF[,1]

```

#Load Libraries
```{r load_libs, tidy=TRUE}
rm(list = ls())
dev.off()
library(plyr)
library(dplyr)
library(lubridate)
library(doMC)
library(caret)
library(reshape)
library(tableone)
library(stargazer)
library(mice)
library(caretEnsemble)
library(ggplot2)

```



#Functions
```{r define functions, tidy=TRUE}


# 6 Functions For reporting results: 
f_get_caret_stack_results <- function (caret_list_data,ensemble_data,test_df,probability=TRUE,Pheno="Control"){
  if (probability == TRUE){
    print("creating probability df")
    model_predi <- lapply(caret_list_data, predict, newdata=test_df, type="prob")
    model_predi <- lapply(model_predi, function(x) x[,Pheno])
    model_predi <- data.frame(model_predi)
    model_predi$ensemble <- predict(ensemble_data, newdata=test_df, type="prob")
    return(model_predi)
  }else{
    print("creating categorical df")
    model_predi <- lapply(caret_list_data, predict, newdata=test_df)
    model_predi <- data.frame(model_predi)
    model_predi$ensemble <- predict(ensemble_data, newdata=test_df)
    model_predi$Phenotype<-test_df[,"Phenotype"]
    return (model_predi)
  }
}

#Function for ConfusionMatrix
f_Confusion_matrix <- function (input_data,Prediction_cols,Pheno_col="Phenotype"){
  Confusiondata<-list()
  for (i in Prediction_cols){
    Confusiondata[[i]]<-confusionMatrix(input_data[,i], input_data[,Pheno_col])
  }
  return(Confusiondata)
}

```




#Define Directories
```{r Define directories}
top_dir<-getwd()
data_dir <-"./P0_Characterise/output/"
data_dir0 <-"./data/"
data_dejong_dir<-"./P00_Characterise_Dejong/output/"
output_dir <-"./P61_Dejong_2way/output/"
figs_dir <-"./P61_Dejong_2way/figs/"
```



# Settings
```{r Define Cores and Seed}
#Allow 8 Cores
registerDoMC(cores = 8) 
#Set Seed
seed = 7
##### Variables
Number_of_Splits <- 10 #Number of splits
#Number_of_Splits <- 2 #Number of splits
Split_Percentage <- 0.8 #Split of data
project_id = "p61_"
project_name = "dejong_data_"
Sub_prefix = "_Step2_"
#Variables for Step 2
#set number of subsets to test
subsets <- c(1:30)*25
#subsets <- c(1:3)*25 
boot_n = 30


```

# Step 1: Load RFE data
```{r Feature Elimination Reporting}

####### load resamples
load(file=paste(output_dir,project_id,project_name,Sub_prefix,"_Resample_List",".rdata",sep=""))

####### Save Plots and Probes.


#Create List of new colnames
Colnames_Plots<-paste("Dataset_",c(1:10),sep="")
#make output list later dataframe.
fullplotdata<-list()
for (Split_n in 1:Number_of_Splits){
  #get data from plot into plotdata
  plotdata<-cbind(data.frame(Resample_list_results$plottestlist[Split_n][[1]]$panel.args[[1]]$y),data.frame(Resample_list_results$plottestlist[Split_n][[1]]$panel.args[[1]]$x))
  #Rename cols
  names(plotdata)<-c("Accuracy","Features")
  #Plot individually
  ptitle<-paste("Recursive Feature Elimination"," (Train Set ",Split_n,")",sep="")  
  ggplot(plotdata[1:30,], aes(x=Features,y=Accuracy, group=1)) +
    geom_line()+
    ggtitle(ptitle)  
  ggsave(paste(figs_dir,project_id,project_name,Sub_prefix,Split_n,"_RFE_linegraph",".png",sep=""))

  #add to overall list.
  fullplotdata[[Split_n]]<-plotdata[1:30,1]

}
#turn to dataframe.
fullplotdata<-data.frame(fullplotdata)
#Add colnames and rownames
colnames(fullplotdata)<-Colnames_Plots
rownames(fullplotdata)<-Resample_list_results$plottestlist[1][[1]]$panel.args[[1]]$x[1:30]
fullplotdata$Features<-Resample_list_results$plottestlist[1][[1]]$panel.args[[1]]$x[1:30]

#Make dataframe 1-10 with only Accuraccy. with x as colnames, remove last col.

#Save Dataframes
#RFE
plotRFE<-melt(fullplotdata, id="Features")
colnames(plotRFE)<-c("Features","Dataset","Accuracy")

#ggplots RFE 
ptitle<-paste("Dejong Recursive Feature Elimination"," of 10 Datasets",sep="")  
ggplot(data.frame(plotRFE), aes(x=Features,y=Accuracy)) +
  geom_line()+
  facet_wrap(~ Dataset, ncol = 2)+
  ggtitle(ptitle)  
ggsave(paste(figs_dir,project_id,project_name,Sub_prefix,"_ALL_Facet_RFE_linegraph",".png",sep=""))

ptitle<-paste("Dejong Recursive Feature Elimination"," of 10 Datasets",sep="")  
ggplot(data.frame(plotRFE), aes(x=Features,y=Accuracy, colour = Dataset  )) +
  geom_line()+
  ggtitle(ptitle)  
ggsave(paste(figs_dir,project_id,project_name,Sub_prefix,"_ALL_Single_RFE_linegraph",".png",sep=""))


#Calculate mean and standard error. To see if data is consistent. 
fullplotdata

#plot that data in ggplot2 line graph. 


#List of all probes in all datasets. Indicate present or not. Remove NAs, to find overlaps.  

####### cut down probes to number of samples. 
for (Split_n in 1:Number_of_Splits){ 
    split<-Resample_list_results$rfelist[[Split_n]]
    if (length(split) > dim(Resample_list_results[[1]])[1]){
      Resample_list_results$rfelist[[Split_n]]<-split[1:dim(Resample_list_results[[1]])[1]]
    }
}

Probe_list_res= Resample_list_results$rfelist
for (Split_n in 1:Number_of_Splits){ 
    split<-Probe_list_res[[Split_n]]
    Probe_list_res[[Split_n]]<-sort(split)
}

Reduce(intersect, Probe_list_res[c(1,3,10)])

####### Save Probes again and plot



```



# Step 3: Load ML data. And Output results
```{r ML data & Results Internal}
#Check How roboust the models are. 


####### Load data
full_data<-list() #empty list wrapper. 
#Makee into function
for (i in 1:Number_of_Splits){
  #fold
  x=i
  #set top split ID
  top_split = paste("Split_",x,sep="")
  print(top_split)
  load(paste(output_dir,project_id,project_name,Sub_prefix,top_split,".rdata",sep=""))
  full_data[[top_split]]<-results_resample
}



#Create Table of performance for all models. 
ResultsTable = list()
TableColOrder<-colnames(data.frame(full_data[[1]]$ensemble_test_performance))
for (i in 1:Number_of_Splits){
  #fold
  x=i
  #set top split ID
  top_split = paste("Split_",x,sep="")
  #get data into df
  tempdf = data.frame(full_data[[i]]$ensemble_test_performance)
  if (length(tempdf) != 7){
    #add to list
    tempdf$pcaNNet=NA
  }  
  rownames(tempdf) = top_split
  ResultsTable[[top_split]] = t(tempdf[,TableColOrder])  

} 
#turn list to df
ResultsTable<-data.frame(ResultsTable)
MLresults_table<-t(round(ResultsTable[,1:10], digits = 2))
rownames(MLresults_table)=paste("AUC_Data_",c(1:10),sep="")
write.csv(MLresults_table,paste(output_dir,project_id,project_name,Sub_prefix,"MLresults.csv",sep=""))


ResultsTable$Model<-rownames(ResultsTable)
#RFE
plot_models<-melt(ResultsTable,id="Model")
colnames(plot_models)<-c("Models","Dataset","AUC")

#Compare Performance with Dataset
ggplot(data.frame(plot_models), aes(x=Dataset,y=AUC, colour=Dataset)) +
  geom_boxplot(notch=F)+
  geom_jitter() + 
  coord_flip()+
  #facet_wrap(as.formula(paste("~", pfacets)))+
  #ggtitle(ptitle)  
ggsave(paste(figs_dir,project_id,project_name,Sub_prefix,"_ROC_vs_Dejong_splits",".png",sep=""))


#Compare Performance by model
ggplot(data.frame(plot_models), aes(x=Models,y=AUC, colour=Models)) +
  geom_boxplot(notch=F)+
  geom_jitter() + 
  coord_flip()+
  #facet_wrap(as.formula(paste("~", pfacets)))+
  #ggtitle(ptitle)  
ggsave(paste(figs_dir,project_id,project_name,Sub_prefix,"_ROC_vs_Dejong_models",".png",sep=""))




summary(full_data[[split]]$ensemble_full)
split = 10
full_data[[split]]$ensemble_test_performance
table(full_data$Split_2$gx_testing[,split])
lapply(full_data$Split_1$ensemble_model_preds_cat,table)
table(full_data$Split_1$ensemble_model_preds_cat[,8])
full_data$Split_2$ensemble_model_preds_cat

full_data[[split]]$ensemble_full

full_data$Split_1$caret_list

full_data$Split_2$ensemble_model_preds_cat

#test caret stack in dejong
results_dejong<-data.frame() #empty list wrapper. 
for (i in 1:Number_of_Splits){
  #fold
  x=i
  #set top split ID
  top_split = paste("Split_",x,sep="")
  print(top_split)
  results_dejong<-rbind(results_dejong,full_data[[top_split]]$ensemble_test_performance)
}


####### Predict Dejong Test


####### Plot Dejong Test results


####### Statistical Tests


```


# Step 4: Load GAP Data and Predict
```{r GAP Predict 2 and 3 cat split}

####### Load GAP
load(file=paste(data_dir,"GX_DF_adj_data.Rdata",sep=""))
#load(file=paste(data_dir,"GX_DF_full_adj_data.Rdata",sep=""))


#Centre and Scale. 
preProcValues <- preProcess(GX_DF_adj, method = c("center", "scale"))
GX_DF_GAP <- predict(preProcValues, GX_DF_adj)
#preProcValues <- preProcess(GX_DF_full_adj, method = c("center", "scale"))
#GX_DF_GAP2 <- predict(preProcValues, GX_DF_full_adj)
dim(GX_DF_GAP)
GX_DF_GAP[1:10,1:10]

####### Clean GAP data
Test_data_1 <-  GX_DF_GAP %>%
      mutate(ICD_DSM = ifelse(is.na(ICD_DSM),"Other_Psychosis",ICD_DSM))


###### Generate GAP results
ResultsTable_GAP = list()
ResultsList_GAP_allpatients = list()
TableColOrder<-colnames(data.frame(full_data[[1]]$ensemble_test_performance))
for (i in 1:Number_of_Splits){
  #set top split ID
  top_split = paste("Split_",i,sep="")
  #Get models for fold
  model_list<-full_data[[top_split]]$caret_list
  gbm_ensemble<-full_data[[top_split]]$ensemble_full
  #Get predictions for fold
  model_preds<-f_get_caret_stack_results (model_list,gbm_ensemble,Test_data_1,probability=TRUE,Pheno="Control")
  #Get class predictions
  model_preds_class<-f_get_caret_stack_results (model_list,gbm_ensemble,Test_data_1,probability=F,Pheno="Control")
  #Change Colnames
  model_preds_class$Phenotype=NULL
  colnames(model_preds_class)=paste(colnames(model_preds_class),"Categorical",sep="_")  
  # Into temp DF
  ResultsList_GAP_allpatients[[top_split]] = cbind(Test_data_1[,1:27],model_preds,model_preds_class)
  tempdf = data.frame(caTools::colAUC(model_preds, Test_data_1$Phenotype))
  # Add pcaNNet if missing
  if (length(tempdf) != 7){
    #add to list
    tempdf$pcaNNet=NA
  }  
  rownames(tempdf) = top_split
  ResultsTable_GAP[[top_split]] = t(tempdf[,TableColOrder])  
}

#turn list to df
ResultsTable_GAP<-data.frame(ResultsTable_GAP)
MLresults_table_GAP<-t(round(ResultsTable_GAP[,1:10], digits = 2))
rownames(MLresults_table_GAP)=paste("Train_Data_",c(1:10),sep="")
write.csv(MLresults_table_GAP,paste(output_dir,project_id,project_name,Sub_prefix,"MLresults_GAP.csv",sep=""))


#Results for all Patients GAP. In list. Save!
save(ResultsList_GAP_allpatients, file=paste(output_dir,project_id,project_name,Sub_prefix,"GAP_Patient_predictions_Saved.rdata",sep=""), compress = T)



###### Make Split 1 
#Function
#Function for Getting Accuracy Table
f_meta_confuse<-function(Input_Data,GAP_Data_subset,Dejong_split = "Split_1"){
  confuse_out<-f_Confusion_matrix(Input_Data,colnames(Input_Data)[35:41])
  #Make lists for main metrics
  temp_dfrows<-names(confuse_out)
  overall_list=list()
  for (i in 1:7){  
    overall_list[["overall"]][[temp_dfrows[i]]]=confuse_out[[temp_dfrows[i]]]$overall
    overall_list[["byClass"]][[temp_dfrows[i]]]=confuse_out[[temp_dfrows[i]]]$byClass
  }

  #Create final DF
  Split_temp_df<-rbind(data.frame(overall_list[["overall"]]),data.frame(overall_list[["byClass"]]))
  Split_temp_df<-data.frame(t(round(Split_temp_df, digits = 3)))
  Split_temp_df$GAP_Data<-GAP_Data_subset
  Split_temp_df$Dejong_data<-Dejong_split
  Split_temp_df$Method<-colnames(Input_Data)[28:34]
  return(Split_temp_df)
}

#Make Data frame for split 1
Split_1_Data<-ResultsList_GAP_allpatients$Split_1

## Full GAP Data
Full_GAP_Metrics_split_1<-f_meta_confuse(Split_1_Data,"Full")

## Remove other psychosis
Split_1_Data_NOP<-filter(Split_1_Data,ICD_DSM !="Other_Psychosis")
GAP_NOP_Metrics_split_1<-f_meta_confuse(Split_1_Data_NOP,"Control_Schizophrenia")

## Remove Schizophrenia
Split_1_Data_OP<-filter(Split_1_Data,ICD_DSM !="Schizophrenia")
GAP_OP_Metrics_split_1<-f_meta_confuse(Split_1_Data_OP,"Control_Other_Psychosis")

#Combine
full_Accuracy_Table<-rbind(Full_GAP_Metrics_split_1,GAP_NOP_Metrics_split_1,GAP_OP_Metrics_split_1)
#Sort
full_Accuracy_Table<-full_Accuracy_Table[,c(19:21,1:18)]
#Save
write.csv(full_Accuracy_Table,paste(output_dir,project_id,project_name,Sub_prefix,"Split_1_Confusion_Matrix_metrics.csv",sep=""))
#Small Table
full_Accuracy_Table




##### Make Bar Plots of data
#Make Plots function
f_boxplot_ML_gx <-function(inputd,xax,yax,col,plot_ylabel,pfacets,ptitle) {
  ggplot(inputd, aes_string(x=xax,y=yax, colour = col)) +
    #geom_point() +  
    geom_boxplot(notch=F)+
    geom_jitter() + 
    coord_flip()+
    labs(x = xax) +  labs(y = plot_ylabel)+
    #facet_wrap(as.formula(paste("~", pfacets)))+
    ggtitle(ptitle)  
}

plot_ylabel = "Ensemble predicted probability of being Control"

#2 Way Boxplot
f_boxplot_ML_gx(Split_1_Data,"Phenotype","ensemble","Phenotype",plot_ylabel,"","")
ggsave(paste(figs_dir,project_id,project_name,Sub_prefix,"_Split_1_Ensemble_Case_Control",".png",sep=""))
#3 Way Boxplot
f_boxplot_ML_gx(Split_1_Data,"ICD_DSM","ensemble","ICD_DSM",plot_ylabel,"","")
ggsave(paste(figs_dir,project_id,project_name,Sub_prefix,"_Split_1_Ensemble_3_Scz_Other_Control",".png",sep=""))

#Boxplots for Ensemble. 4 things to look at. 1: Case vs Control. 2: 3-Way.  
Split_1_Data


#Plots of Algorithm for all splits
gglist<-list()
for (i in 1:Number_of_Splits){
  #set top split ID
  top_split = paste("Split_",i,sep="")
  gglist[[top_split]]=ggplot(data = ResultsList_GAP_allpatients[[top_split]], aes(x = ICD_DSM, y=ensemble, colour = ICD_DSM, fill = ICD_DSM)) +
    geom_boxplot(alpha = 0)+
    geom_jitter(alpha = 0.3)+
    coord_flip()+
    facet_wrap(~Ethnicity)+
    ggtitle(paste(top_split,"GAP data Predictions","Glmnet","Model",sep=" "))
}
gglist




#combined predictions and real values
ggplot(data = model_preds3, aes(x = ICD_DSM, y=glmnet, colour = ICD_DSM, fill = ICD_DSM)) +
  geom_boxplot(alpha = 0)+
  geom_jitter(alpha = 0.3)+
  coord_flip()+
  ggtitle("Glmnet Model Trained on Dejong Predictions in GAP data")


ggplot(data = filter(model_preds3,ICD_DSM =="Other_Psychosis"), aes(x = ICD10_GAP, y=glmnet, colour = ICD10_GAP, fill = ICD10_GAP)) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  geom_boxplot(alpha = 0)+
  geom_jitter(alpha = 0.3)+
  coord_flip()+
  ggtitle("Glmnet Model Trained on Dejong Predictions on Other Psychosis data")


ggplot(data = model_preds3, aes(x = ICD_DSM, y=glmnet, colour = ICD_DSM, fill = ICD_DSM)) +
  geom_boxplot(alpha = 0)+
  geom_jitter(alpha = 0.3)+
  facet_wrap(~ Ethnicity)+
  coord_flip()+
  ggtitle("Glmnet Model Trained on Dejong Predictions in GAP data")

ggplot(data = filter(model_preds3,ICD_DSM =="Other_Psychosis"), aes(x = ICD10_GAP, y=glmnet, colour = ICD10_GAP, fill = ICD10_GAP)) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  geom_boxplot(alpha = 0)+
  geom_jitter(alpha = 0.3)+
  facet_wrap(~ Ethnicity)+
  coord_flip()+
  ggtitle("Glmnet Model Trained on Dejong Predictions on Other Psychosis data")



####### Predict GAP


####### Plot GAP results


####### Plot 3 way results


####### Statistical Tests


```

# Step 5: GAP data post hoc exploration
```{r GAP Post Hoc}


```

# Step 6: Load IMPACT & Predict
```{r IMPACT}


```