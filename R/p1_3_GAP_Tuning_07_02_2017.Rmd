---
title: "Feature Selection"
author: "DJL"
date: "02/10/2016"
output:
  html_document:
    toc: yes
    toc_float: yes
---

#Libraries
```{r load_libs, tidy=TRUE}
rm(list = ls())
dev.off()
library(plyr)
library(dplyr)
library(randomForest)
library(lubridate)
library(doMC)
library(caret)
library(reshape)


```

#Functions
```{r define functions, tidy=TRUE}

#all machine learning models
ml_fun<- function(dataset) {
  # SVM Poly
  set.seed(seed)
  fit.svmPoly <- train(Phenotype~., data=dataset, method="svmPoly", metric=metric, preProc=c("center", "scale"), trControl=control, fit=FALSE)
  # SVM Linear
  set.seed(seed)
  fit.svmLinear <- train(Phenotype~., data=dataset, method="svmLinear2", metric=metric, preProc=c("center", "scale"), trControl=control, fit=FALSE)
  # Random Forest
  set.seed(seed)
  fit.rf <- train(Phenotype~., data=dataset, method="rf", metric=metric, trControl=control)
  # Stochastic Gradient Boosting (Generalized Boosted Modeling)
  set.seed(seed)
  fit.gbm <- train(Phenotype~., data=dataset, method="gbm", metric=metric, trControl=control, verbose=FALSE)
  ##Make List of models
  modellist<-list(svmPoly=fit.svmPoly, rf=fit.rf, gbm=fit.gbm)
  modellist
}




```



#Directories
```{r Define directories}
data_dir <-"./P0_Characterise/output/"
P1_output_dir <-"./P1_Hypothesis_Free/output/"
P1_figs_dir <-"./P1_Hypothesis_Free/figs/"

```

#Load Data
```{r Load data}

load(paste(data_dir,"Train_and_Test_data.Rdata",sep=""))
load(file=paste(P1_output_dir,"RFE_model_results_hypothesis_free_data.Rdata",sep=""))


```

#Configure
```{r Define Cores and Seed}

#Allow 8 Cores
registerDoMC(cores = 8) # Note that there might be minor issues with reproducibility due to the cores that are used. 
#Set Seed
seed = 7
set.seed(seed)

###### Control for algorithms #########
# Random Forrest
rfcontrol <- trainControl(method="repeatedcv", number=10, repeats=3, sampling="down", savePredictions="all")
rftunegrid <- expand.grid(.mtry=(1:20)*2)
# GBM
GBMcontrol <- trainControl(method="repeatedcv", number=10, repeats=3, sampling="down", savePredictions="all")


#Define Metric to choose best model
metric <- "Accuracy"

#Names of all predictors to be used. 
opt_rfepredictors<-predictors(rfe_output)


#input data frame
expressionRFE<-training_df[,c("Phenotype",opt_rfepredictors)]
#expressionRFE<-training_df[,c("Phenotype",opt_rfepredictors[1:50])]


```

#Random Forrest Tuning
```{r Random Forrest Tuning}



#Run Parameter Search.
set.seed(seed)
RF_search <- train(Phenotype~., data=expressionRFE, method="rf", metric=metric, tuneGrid=rftunegrid, trControl=rfcontrol) 
#takes around 15 minutes with 8 cores and 60 runs at 30 x cross validation. 

#Summary and Plot. 
RF_search

ggplot(RF_search)  
ggplot(RF_search, metric = "Kappa")  


```

#Stochastic Gradient Boosting, Parameter Search
```{r GBM Parameter Search}

#tuning grid GBM
gbmGrid <-  expand.grid(interaction.depth = (1:10)*3, n.trees = (1:15)*100, shrinkage = 0.1, n.minobsinnode = 20)

#Set seed and run model
set.seed(seed)
gbmFit <- train(Phenotype ~ ., data = expressionRFE, method = "gbm",trControl = GBMcontrol, verbose = FALSE,tuneGrid = gbmGrid)
gbmFit


ggplot(gbmFit)  
ggplot(gbmFit, metric = "Kappa")  




trellis.par.set(caretTheme())
plot(gbmFit, metric = "Kappa", plotType = "level",
     scales = list(x = list(rot = 90)))

```


#SVM Poly, Parameter Search
```{r SVMpoly Parameter Search}


```


#Save all models for reference
```{r save}


```
