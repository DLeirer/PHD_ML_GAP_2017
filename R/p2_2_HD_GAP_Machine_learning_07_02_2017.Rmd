---
title: "Machine Learning"
author: "DJL"
date: "02/10/2016"
output:
  html_document:
    toc: yes
    toc_float: yes
---

#Overview

**Aim**  
Create Machine learning models using GAP data. 


#Libraries
```{r load_libs, tidy=TRUE}
rm(list = ls())
dev.off()
library(plyr)
library(dplyr)
library(randomForest)
library(lubridate)
library(doMC)
library(caret)
library(reshape)
library(tableone)
```

#Functions
```{r define functions, tidy=TRUE}

#all machine learning models
ml_fun<- function(dataset) {
  # Linear Discriminant Analysis
  set.seed(seed)
  fit.lda <- train(Phenotype~., data=dataset, method="lda", metric=metric, preProc=c("center", "scale"), trControl=control)
  # Logistic Regression
  set.seed(seed)
  fit.glm <- train(Phenotype~., data=dataset, method="glm", metric=metric, trControl=control)
  # GLMNET
  set.seed(seed)
  fit.glmnet <- train(Phenotype~., data=dataset, method="glmnet", metric=metric, preProc=c("center", "scale"), trControl=control)
  # SVM Radial
  set.seed(seed)
  fit.svmRadial <- train(Phenotype~., data=dataset, method="svmRadial", metric=metric, preProc=c("center", "scale"), trControl=control, fit=FALSE)
  # SVM Poly
  set.seed(seed)
  fit.svmPoly <- train(Phenotype~., data=dataset, method="svmPoly", metric=metric, preProc=c("center", "scale"), trControl=control, fit=FALSE)
  # SVM Linear
  set.seed(seed)
  fit.svmLinear <- train(Phenotype~., data=dataset, method="svmLinear2", metric=metric, preProc=c("center", "scale"), trControl=control, fit=FALSE)
  # kNN
  set.seed(seed)
  fit.knn <- train(Phenotype~., data=dataset, method="knn", metric=metric, preProc=c("center", "scale"), trControl=control)
  # Naive Bayes
  set.seed(seed)
  fit.nb <- train(Phenotype~., data=dataset, method="nb", metric=metric, trControl=control)
  # C5.0
  set.seed(seed)
  fit.c50 <- train(Phenotype~., data=dataset, method="C5.0", metric=metric, trControl=control)
  # Random Forest
  set.seed(seed)
  fit.rf <- train(Phenotype~., data=dataset, method="rf", metric=metric, trControl=control)
  # Stochastic Gradient Boosting (Generalized Boosted Modeling)
  set.seed(seed)
  fit.gbm <- train(Phenotype~., data=dataset, method="gbm", metric=metric, trControl=control, verbose=FALSE)

  ##Make List of models
  modellist<-list(lda=fit.lda, logistic=fit.glm, glmnet=fit.glmnet,svmRad=fit.svmRadial,svmPoly=fit.svmPoly,svmLinear=fit.svmLinear, knn=fit.knn, nb=fit.nb,c50=fit.c50, rf=fit.rf, gbm=fit.gbm)
  modellist
}


#heatmap plotting function
heatmap_ml<- function(model="NA",x_ax="NA",x_title="NA", y_ax="NA",y_title="NA",color_metric="NA",h_title="NA",save_output="NA") {
  model_data<-model$results
  model_data[,x_ax]<-as.factor(model_data[,x_ax])
  model_data[,y_ax]<-as.factor(model_data[,y_ax])
  model_data_metric<-model_data[,color_metric]
  print(str(model_data))
  ggplot(data = model_data, aes_string(x=x_ax, y=y_ax, fill=color_metric))+
    geom_tile(color = "white")+
    scale_fill_gradient2(low = "red", high = "blue", mid = "white", 
      midpoint = mean(model_data_metric), limit = c(min(model_data_metric),max(model_data_metric)), space = "Lab", 
      name="") +
    theme_minimal()+ 
    theme(axis.text.x = element_text(angle = 90, vjust = 1, 
      size = 10, hjust = 1))+
    coord_fixed()+
    ggtitle(h_title) + xlab(x_title) + ylab(y_title)
  ggsave(save_output)
}

#find simple model. 
simple_model_fun<- function(ml_model,tol_percentage = 1, metric_sm=metric) {
    tolerance(ml_model$results, metric = metric_sm, tol = tol_percentage, maximize = TRUE)    
}


```


#Directories
```{r Define directories}
data_dir <-"./P0_Characterise/output/"
output_dir <-"./P2_Hypothesis_Driven/output/"
figs_dir <-"./P2_Hypothesis_Driven/figs/"

#Pre and post fixes.
project_name = "Hypothesis_Driven"
project_id = "p2_2"

```




#Load Data
```{r Load data}

load(paste(data_dir,"Train_and_Test_data.Rdata",sep=""))
load(file=paste(output_dir,"RFE_model_results_hypothesis_free_data.Rdata",sep=""))


```

#Configure
```{r Define Cores and Seed}

#Allow 8 Cores
registerDoMC(cores = 8) 
#Set Seed
seed = 7
set.seed(seed)

#Control for algorithms
control <- trainControl(method="repeatedcv", number=10, repeats=10, sampling="down", savePredictions="all") ## Steve added sampling argument. READ!!

#Define Metric to choose best model
metric <- "Accuracy"

#Names of all predictors to be used. 
opt_rfepredictors<-predictors(rfe_output)


#input data frame
expressionRFE<-training_df[,c("Phenotype",opt_rfepredictors)]



```

# Machine Learning
```{r feature selection}

#Warning will use a lot of resources and take a long time. Output is a list of machine learning models. 
ml_models<-ml_fun(expressionRFE)

#Save Models
save(ml_models, file=paste(output_dir,"Classification_models.rdata",sep=""), compress = T)

```



# PLotting
```{r Plotting of results}

###########################Use classification_model_script_gap_and_impact.rmd


#extract accuracy Models
ml_resample<-resamples(ml_models)
Acc_results<-ml_resample$values[,grep("*Accuracy",colnames(ml_resample$values))]

## Plot 

#single point
s_title=
Accuracy_results<-sapply(Acc_results,mean)
accuracy_plot<-as.data.frame(Accuracy_results)
accuracy_plot<-as.data.frame(Accuracy_results)
accuracy_plot$Algorithm<-row.names(accuracy_plot)

s_title="Dotplot_comparing"
ggplot(accuracy_plot,
  aes(x=Accuracy_results,y=Algorithm,color=Algorithm)) +
  geom_point()+ 
  ggtitle("Average accuracy of 10x10-fold cross validation for 11 Models")
ggsave(paste(figs_dir,project_id,"_",s_title,"_",project_name,"_Free.png",sep=""))
results<-ml_resample

#all points
Accuracy_results<-ml_resample$values[,grep("*Accuracy",colnames(ml_resample$values))]
accuracy_plot<-Accuracy_results[,row.names(as.matrix(sort(sapply(Accuracy_results,mean),decreasing = F)))]
accuracy_plot$Datasets<-c("RFE(GAP)")
accuracy_plot2<-melt(accuracy_plot, id="Datasets")
colnames(accuracy_plot2)<-c("GeneLists","Algorithm","Accuracy")

table(accuracy_plot2[,3])
s_title="Dotplot_comparing_cv"
ggplot(accuracy_plot2,
  aes(x=Algorithm,y=Accuracy, colour = Algorithm)) +
  geom_point() + 
  geom_boxplot()+
  coord_flip()+
  ggtitle("Accuracy of 10x10-fold cross validation for 11 Models")
ggsave(paste(figs_dir,project_id,"_",s_title,"_",project_name,"_Free.png",sep=""))


```

#Tuning of top models Random. 
```{r Tuning}

#change project id
project_id = "p2_3"
#Define tune parameters and depth (careful, this can be really ineffective for large numbers of hyperparameters.)
tunecontrol <- trainControl(method="repeatedcv", number=10, repeats=5, sampling="down", savePredictions="all")
tunelength <- 20

#Run Parameter Search.
set.seed(seed)
RF_fit <- train(Phenotype~., data=expressionRFE, method="rf", metric=metric, trControl=tunecontrol,tuneLength=tunelength) 
set.seed(seed)
nb_fit <-train(Phenotype~., data=expressionRFE, method="nb", metric=metric, tuneLength=tunelength, trControl=tunecontrol)
set.seed(seed)
c50_fit <-train(Phenotype~., data=expressionRFE, method="C5.0", metric=metric, tuneLength=tunelength, trControl=tunecontrol)

sq
#Plots
s_title="RF_tuning"
ggplot(RF_fit)+
  ggtitle("Random Forrest Grid Search (Accuracy)")
ggsave(paste(figs_dir,project_id,"_",s_title,"_",project_name,"_Free.png",sep=""))

s_title="c50_tuning"
ggplot(c50_fit)+
  ggtitle("c50 Grid Search (Accuracy)")
ggsave(paste(figs_dir,project_id,"_",s_title,"_",project_name,"_Free.png",sep=""))


```



#Find low complexity model Here we look for a balance between complexity and accuracy. 
```{r complexity vs accuracy}


#####   Find least complex model within 1 percent of top performance
simpleRF<-as.numeric(RF_fit$results[simple_model_fun(ml_model=RF_fit),][1,])
simplec50<-as.numeric(c50_fit$results[simple_model_fun(ml_model=c50_fit),][1,])
simple_model_fun(ml_model=c50_fit)

#####   Define Grid searches. Etc. 
fcontrol <- trainControl(method="repeatedcv", number=10, repeats=3, sampling="down", savePredictions="all")


rftGrid <- expand.grid(.mtry=simpleRF[1])
gbmGrid <- expand.grid(shrinkage = simpleGBM[1],interaction.depth = simpleGBM[2],n.minobsinnode = simpleGBM[3], n.trees = simpleGBM[4])
svmGrid <-  expand.grid(degree = simplePoly[1], scale = simplePoly[2],C = simplePoly[3])

predict(RF_fit$results[4,], newdata = testing_df)
predict(RF_fit, newdata = testing_df)
predict(RF_fit, newdata = testing_df, type="prob")



train(Phenotype ~ ., data = expressionRFE, method = "gbm", metric = metric, trControl = GBMcontrol, verbose = FALSE,tuneGrid = gbmGrid)
#####   Run models
set.seed(seed)
RF_final <- train(Phenotype~., data=expressionRFE, method="rf", metric=metric, trControl= fcontrol, verbose = FALSE, tuneGrid=rftGrid) 
set.seed(seed)
gbm_final <- train(Phenotype ~ ., data = expressionRFE, method = "gbm", metric = metric, trControl = fcontrol, verbose = FALSE, tuneGrid = gbmGrid)
set.seed(seed)
svmPoly_final <- train(Phenotype ~ ., data = expressionRFE, method = "svmPoly", metric = metric, trControl = fcontrol, verbose = FALSE, preProc = c("center", "scale"), tuneGrid = svmGrid)


#variable Importance
importance <- varImp(RF_fit, scale=FALSE)
print(importance)
write.table(print(importance$importance), file=paste(P1_output_dir,"p1_3_RF_model_simple_varImp.tsv",sep=""),row.names=T,quote=FALSE,sep = "\t")



```


#Resample of top performers and top low complexity performers. 
```{r complexity vs accuracy}
High_Low_complexity_model_list<-list(RF = RF_search, RF_simple=RF_final, GBM = gbmFit, GBM_simple = gbm_final, svmPoly = svmPolyFit2, svmPoly_simple = svmPoly_final)

#Summary and Plot. 
ml_resample<-resamples(High_Low_complexity_model_list)
resamps=ml_resample

```

#Compare best models
```{r Compare}
str(resamps)
#define names of models for plot
Model_names<-c("RF_Complex","RF_Simple","GBM_Complex","GBM_Simple","svmPoly_Complex","svmPoly_Simple")
#plot comparing models
Accuracy_results<-resamps$values[,grep("*Accuracy",colnames(resamps$values))]
colnames(Accuracy_results)<-Model_names
accuracy_plot<-Accuracy_results[,row.names(as.matrix(sort(sapply(Accuracy_results,mean),decreasing = F)))]
accuracy_plot$Datasets<-c("RFE(GAP)")
accuracy_plot2<-melt(accuracy_plot, id="Datasets")
colnames(accuracy_plot2)<-c("GeneLists","Algorithm","Accuracy")

accuracy_plot2

title_final_models<-"Comparision of High and Low Complexity Models"
ggplot(accuracy_plot2,
       aes(x=Algorithm,y=Accuracy, colour = Algorithm)) +
  geom_point() + 
  geom_boxplot()+
  geom_jitter()+
  coord_flip()+
  ggtitle(title_final_models)
ggsave(paste(P1_figs_dir,"p1_3_Hi_Lo_complexity_model_comparision_.png",sep=""))


```

#Save all models for reference
```{r save}


#Save Models
save(High_Low_complexity_model_list, file=paste(P1_output_dir,"P1_3_High_Low_complexity_model_list.rdata",sep=""), compress = T)

```



#Validation
```{r Validation}

#Predictions
predict_fit<-predict(gbmFit, newdata = testing_df)
confusionMatrix(predict_fit, testing_df$Phenotype)

predict_fit<-predict(gbm_final, newdata = testing_df)
confusionMatrix(predict_fit, testing_df$Phenotype)


predict_fit<-predict(RF_search, newdata = testing_df)
confusionMatrix(predict_fit, testing_df$Phenotype)

predict_fit<-predict(RF_final, newdata = testing_df)
confusionMatrix(predict_fit, testing_df$Phenotype)


predict_fit<-predict(svmPolyFit2, newdata = testing_df)
confusionMatrix(predict_fit, testing_df$Phenotype)

predict_fit<-predict(svmPoly_final, newdata = testing_df)
confusionMatrix(predict_fit, testing_df$Phenotype)


```



## Check most important variables for GAP
```{r}

#use model to predict training data
predict_train_fit<-predict.train(RF_fit, newdata = testing_df)
confusionMatrix(predict_train_fit, testing_df$Phenotype)

#check variables
names(testing_df)[1:26]

#create object to be used from now on. 
full_data<-testing_df[,1:26]


#add rownames to predictions
tpredictions<-data.frame(predict_train_fit)
rownames(tpredictions)<-full_data$sampleID


#create test data with pheno predictions. 
model_test_pheno<-merge(tpredictions,full_data,by.x="row.names",by.y="sampleID")
## Make confusion matrix column
model_test_pheno<-model_test_pheno %>% mutate(ConfusionMatrix=ifelse(Phenotype == "FEP" & predict_train_fit=="control","FEP_misclassed",
                                            ifelse(Phenotype == "FEP" & predict_train_fit=="FEP","FEP_true",
                                            ifelse(Phenotype != "FEP" & predict_train_fit=="FEP","Control_misclassified","Control_true"))))
# Center and Scale PRS
model_test_pheno[19:27]<-scale(model_test_pheno[19:27])


```


## Create Table of variables
```{r}


#Create Table of demographics with 4 predicted groups.
t_title="RF_model_Table_CM_1"
listVars <- c("Gender","Age", "Ethnicity","BMI","Tobacco",names(model_test_pheno)[19:27])
catVars <- c("Ethnicity","Tobacco")
table1_full <- CreateTableOne(vars = listVars, data = model_test_pheno, factorVars = catVars,strata=c("ConfusionMatrix"),includeNA = T)
table1print<-print(table1_full)
write.table(table1print, file=paste(output_dir,project_id,"_",t_title,"_","Demographics.csv",sep=""),row.names = TRUE, col.names = TRUE,quote=FALSE,sep = "\t")


#Create Table of demographics with 4 predicted groups. 
t_title="RF_model_Table_CM_1_meds"
listVars2 <- c("Medication","dsmiv.opcrit","icd10.opcrit","PanssScore","PanssPositive","PanssNegative","PanssPsycho")
catVars2 <- c("Medication","dsmiv.opcrit","icd10.opcrit")
table2_full <- CreateTableOne(vars = listVars2, data = filter(model_test_pheno,Phenotype=="FEP"), factorVars = catVars2,strata=c("ConfusionMatrix"),includeNA = T)
table2print<-print(table2_full)
write.table(table2print, paste(output_dir,project_id,"_",t_title,"_","Demographics.csv",sep=""),row.names = TRUE, col.names = TRUE,quote=FALSE,sep = "\t")



```



## Create Table of variables
```{r}

dim(model_test_pheno)
FEP_data<-filter(model_test_pheno,Phenotype=="FEP")


SUMO3<-testing_df$SUMO3

FEP_data<-testing_df
FEP_data<-merge(tpredictions,FEP_data,by.x="row.names",by.y="sampleID")




FEP_data<-filter(FEP_data,Phenotype=="FEP")
FEP_data<-filter(FEP_data,Phenotype=="FEP")
FEP_data<-FEP_data[complete.cases(FEP_data$Pol_1_GAP_all_strict_excl_WTCCC2),]
FEP_data[19:27]<-scale(FEP_data[19:27])
corPRS_genes<-cor(FEP_data[,-c(1:26)], method = c("pearson"))

?sort
sort(corPRS_genes[1,],decreasing = T)["SUMO3"]

```
