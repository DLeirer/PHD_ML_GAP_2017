---
title: "Feature Selection"
author: "DJL"
date: "02/10/2016"
output:
  html_document:
    toc: yes
    toc_float: yes
---

#Libraries
```{r load_libs, tidy=TRUE}
rm(list = ls())
dev.off()
library(plyr)
library(dplyr)
library(randomForest)
library(lubridate)
library(doMC)
library(caret)
library(reshape)


```

#Libraries
```{r load_libs, tidy=TRUE}
heatmap_ml<- function(model="NA",x_ax="NA",x_title="NA", y_ax="NA",y_title="NA",color_metric="NA",h_title="NA",save_output="NA") {
  model_data<-model$results
  model_data[,x_ax]<-as.factor(model_data[,x_ax])
  model_data[,y_ax]<-as.factor(model_data[,y_ax])
  model_data_metric<-model_data[,color_metric]
  print(str(model_data))
  ggplot(data = model_data, aes_string(x=x_ax, y=y_ax, fill=color_metric))+
    geom_tile(color = "white")+
    scale_fill_gradient2(low = "red", high = "blue", mid = "white", 
      midpoint = mean(model_data_metric), limit = c(min(model_data_metric),max(model_data_metric)), space = "Lab", 
      name="") +
    theme_minimal()+ 
    theme(axis.text.x = element_text(angle = 90, vjust = 1, 
      size = 10, hjust = 1))+
    coord_fixed()+
    ggtitle(h_title) + xlab(x_title) + ylab(y_title)
  ggsave(save_output)
}


```


#Directories
```{r Define directories}
data_dir <-"./P0_Characterise/output/"
P1_output_dir <-"./P1_Hypothesis_Free/output/"
P1_figs_dir <-"./P1_Hypothesis_Free/figs/"

```

#Load Data
```{r Load data}

load(paste(data_dir,"Train_and_Test_data.Rdata",sep=""))
load(file=paste(P1_output_dir,"RFE_model_results_hypothesis_free_data.Rdata",sep=""))


```

#Configure
```{r Define Cores and Seed}

#Allow 8 Cores
registerDoMC(cores = 8) # Note that there might be minor issues with reproducibility due to the cores that are used. 
#Set Seed
seed = 7
set.seed(seed)


#Define Metric to choose best model
metric <- "Accuracy"

#Names of all predictors to be used. 
opt_rfepredictors<-predictors(rfe_output)


#input data frame
expressionRFE<-training_df[,c("Phenotype",opt_rfepredictors)]

```

#Random Forrest Tuning
```{r Random Forrest Tuning}

#Random Forrest
rfcontrol <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid", sampling="down", savePredictions="all")
rftunegrid <- expand.grid(.mtry=(1:30))


#Run Parameter Search.
set.seed(seed)
RF_search <- train(Phenotype~., data=expressionRFE, method="rf", metric=metric, tuneGrid=rftunegrid, trControl=rfcontrol) 

#Summary and Plot. 
RF_search

```

#Stochastic Gradient Boosting, Parameter Search
```{r GBM Parameter Search}

# GBM
GBMcontrol <- trainControl(method="repeatedcv", number=10, repeats=3, sampling="down", savePredictions="all")


#tuning grid GBM
gbmGrid <-  expand.grid(interaction.depth = ((1:8)*2)-1, n.trees = (1:20)*100, shrinkage = 0.1, n.minobsinnode = 20)

#Set seed and run model
set.seed(seed)
gbmFit <- train(Phenotype ~ ., data = expressionRFE, method = "gbm", metric = metric, trControl = GBMcontrol, verbose = FALSE,tuneGrid = gbmGrid)
gbmFit


```


#SVM Poly, Parameter Search
```{r SVMpoly Parameter Search}


#tuning grid SVMpoly
svmControl <- trainControl(method="repeatedcv", number=10, repeats=3,search="random", sampling="down", savePredictions="all")
svmGrid <-  expand.grid(degree = 3, scale = c(4^(-5:2)),C = c(4^(-2:5)))

#Set seed and run model
set.seed(seed)
svmPolyFit <- train(Phenotype ~ ., data = expressionRFE, method = "svmPoly", metric = metric, trControl = svmControl, verbose = FALSE, preProc = c("center", "scale"), tuneGrid = svmGrid )
svmPolyFit


#tuning grid SVMpoly second round informed by Grid search
svmGrid <-  expand.grid(degree = 3, scale = c((1:10)/1000),C = c(2^(-5:1),3,4))

#Set seed and run model
set.seed(seed)
svmPolyFit2 <- train(Phenotype ~ ., data = expressionRFE, method = "svmPoly", metric = metric, trControl = svmControl, verbose = FALSE, preProc = c("center", "scale"), tuneGrid = svmGrid )
class(svmPolyFit2)



```

#Plot for models
```{r plots}

################## RF  ############################  

ggplot(RF_search)+
  ggtitle("Random Forrest Grid Search (Accuracy)")
ggsave(paste(P1_figs_dir,"p1_3_RF_accuracy_.png",sep=""))

ggplot(RF_search, metric = "Kappa")+
  ggtitle("Random Forrest Grid Search (Kappa)")
ggsave(paste(P1_figs_dir,"p1_3_RF_Kappa_.png",sep=""))


################## GBM  ############################ 
#Grid search broad search. 


ggplot(gbmFit)+ggtitle("Stochastic Gradient Boosting Grid Search Results (Accuracy)")
ggsave(paste(P1_figs_dir,"p1_3_GBM_accuracy_.png",sep=""))

ggplot(gbmFit, metric = "Kappa")+ggtitle("Stochastic Gradient Boosting Grid Search Results (Kappa)")
ggsave(paste(P1_figs_dir,"p1_3_GBM_Kappa_.png",sep=""))


c_metric = "Accuracy"
svmTitle=paste("Stochastic Gradient Boosting Grid Search Results ","(",c_metric,")",sep="")
svmSave=paste(P1_figs_dir,"p1_3_GBM_",c_metric,"_heatmap.jpg",sep="")
heatmap_ml(model=gbmFit,x_ax="n.trees",x_title="Boosting Iterations", y_ax="interaction.depth", y_title="Max Tree Depth",color_metric=c_metric,h_title=svmTitle,save_output=svmSave )

c_metric = "Kappa"
svmTitle=paste("Stochastic Gradient Boosting Grid Search Results ","(",c_metric,")",sep="")
svmSave=paste(P1_figs_dir,"p1_3_GBM_",c_metric,"_heatmap.jpg",sep="")
heatmap_ml(model=gbmFit,x_ax="n.trees",x_title="Boosting Iterations", y_ax="interaction.depth", y_title="Max Tree Depth",color_metric=c_metric,h_title=svmTitle,save_output=svmSave )



################## SVM poly ############################ 
#Grid search broad search. 



c_metric = "Accuracy"
svmTitle=paste("SVM Polynomial intial Grid Search Results ","(",c_metric,")",sep="")
svmSave=paste(P1_figs_dir,"p1_3_svmPoly_",c_metric,"_screen_heatmap.jpg",sep="")
heatmap_ml(model=svmPolyFit,x_ax="scale",x_title="Scale", y_ax="C", y_title="Cost",color_metric=c_metric,h_title=svmTitle,save_output=svmSave )

c_metric = "Kappa"
svmTitle=paste("SVM Polynomial intial Grid Search Results ","(",c_metric,")",sep="")
svmSave=paste(P1_figs_dir,"p1_3_svmPoly_",c_metric,"_screen_heatmap.jpg",sep="")
heatmap_ml(model=svmPolyFit,x_ax="scale",x_title="Scale", y_ax="C", y_title="Cost",color_metric=c_metric,h_title=svmTitle,save_output=svmSave )

#Grid search closer look. 

c_metric = "Accuracy"
svmTitle=paste("SVM Polynomial Final Grid Search Results ","(",c_metric,")",sep="")
svmSave=paste(P1_figs_dir,"p1_3_svmPoly_",c_metric,"_final_heatmap.jpg",sep="")
heatmap_ml(model=svmPolyFit2,x_ax="scale",x_title="Scale", y_ax="C", y_title="Cost",color_metric=c_metric,h_title=svmTitle,save_output=svmSave )

c_metric = "Kappa"
svmTitle=paste("SVM Polynomial Final Grid Search Results ","(",c_metric,")",sep="")
svmSave=paste(P1_figs_dir,"p1_3_svmPoly_",c_metric,"_final_heatmap.jpg",sep="")
heatmap_ml(model=svmPolyFit2,x_ax="scale",x_title="Scale", y_ax="C", y_title="Cost",color_metric=c_metric,h_title=svmTitle,save_output=svmSave )

```

#Create best performing models
```{r Compare}

gbmFit
RF_search
svmPolyFit2

#Find least complex model within 1 percent of top performance
whichTwoPct <- tolerance(RF_search$results, metric = metric, 
                         tol = 1, maximize = TRUE)  
RF_search$results[whichTwoPct,1:3]

whichTwoPct <- tolerance(gbmFit$results, metric = metric, 
                         tol = 1, maximize = TRUE)  
gbmFit$results[whichTwoPct,1:6]

whichTwoPct <- tolerance(svmPolyFit2$results, metric = metric, 
                         tol = 1, maximize = TRUE)  
svmPolyFit2$results[whichTwoPct,1:6]




#Random Forrest
rfcontrol <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid", sampling="down", savePredictions="all")
rftunegrid <- expand.grid(.mtry=(2))


#Run Parameter Search.
set.seed(seed)
RF_search_final <- train(Phenotype~., data=expressionRFE, method="rf", metric=metric, tuneGrid=rftunegrid, trControl=rfcontrol) 

#Summary and Plot. 
RF_search_final$results
ml_resample<-resamples(RF_search_final)



```

#Compare best models
```{r Compare}
resamps <- resamples(list(GBM = gbmFit,
                          RF = RF_search,
                          svmPoly = svmPolyFit2))
resamps


#plot comparing models
Accuracy_results<-resamps$values[,grep("*Accuracy",colnames(resamps$values))]
accuracy_plot<-Accuracy_results[,row.names(as.matrix(sort(sapply(Accuracy_results,mean),decreasing = F)))]
accuracy_plot$Datasets<-c("RFE(GAP)")
accuracy_plot2<-melt(accuracy_plot, id="Datasets")
colnames(accuracy_plot2)<-c("GeneLists","Algorithm","Accuracy")

accuracy_plot2

table(accuracy_plot2[,3])

ggplot(accuracy_plot2,
       aes(x=Algorithm,y=Accuracy, colour = Algorithm)) +
  geom_point() + 
  geom_boxplot()+
  coord_flip()+
  ggtitle("Accuracy of 10x10-fold cross validation for 11 Models")


summary(resamps)
trellis.par.set(theme1)
bwplot(resamps, layout = c(2, 1))

trellis.par.set(caretTheme())
dotplot(resamps, metric = "Accuracy")
trellis.par.set(theme1)
xyplot(resamps, what = "BlandAltman")
splom(resamps)

difValues <- diff(resamps)
difValues
summary(difValues)

trellis.par.set(caretTheme())
bwplot(difValues, layout = c(2, 1))

trellis.par.set(caretTheme())
dotplot(difValues)

ggplot(difValues)

```

#Save all models for reference
```{r save}

```

