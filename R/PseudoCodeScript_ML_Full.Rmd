---
title: "Machine Learning"
author: "DJL"
date: "02/10/2016"
output:
  html_document:
    toc: yes
    toc_float: yes
---

#Overview

**Aim**  
Create Machine learning models using GAP data. 


#Load Libraries
```{r load_libs, tidy=TRUE}
rm(list = ls())
dev.off()
library(plyr)
library(dplyr)
library(lubridate)
library(doMC)
library(caret)
library(reshape)
library(tableone)
library(stargazer)
library(mice)


```

#Functions
```{r define functions, tidy=TRUE}




```


#Define Directories
```{r Define directories}
top_dir<-getwd()
data_dir <-"./P0_Characterise/output/"
data_dir0 <-"./data/"
data_dejong_dir<-"./P00_Characterise_Dejong/output/"
output_dir <-"./P51_GAP_2way/output/"
figs_dir <-"./P51_GAP_2way/figs/"
```

#Load Data
```{r Load data}

load(file=paste(data_dejong_dir,"Dejong_reduced_Gx_data_and_pheno.RData",sep=""))
load(file=paste(data_dir,"GX_DF_adj_data.Rdata",sep=""))


#Reduce Gene expression GAP to mirror Dejong. 
GAP_Fdata<-filter(GAP_Dejong_Full_Fdata,GAP_GAPDJ==TRUE)

#Remove all HS and Loc
GAP_Fdata <- GAP_Fdata %>% 
  mutate(LOC_HS_DROP=ifelse( grepl("^LOC",TargetID),"DROP",
                       ifelse( grepl("^HS\\.",TargetID), "DROP","KEEP"))) 
GAP_Fdata<-filter(GAP_Fdata,LOC_HS_DROP == "KEEP")

Gx_DF_1_locHS<-GX_DF_adj[,c("Phenotype",GAP_Fdata$TargetID)]



```

# Step 1: Split Data 
```{r Define Cores and Seed}
############Step 1 Functions
##### get training or testing DF 
#Get train or test DF function
f_train_test <- function (Data ,indexDF ,index ,Training = TRUE){
  training_index <- DataPartition_DF[,index]  
  if (Training == TRUE){
    print("Training dataframe created")
    training <- Data[training_index,]
    return (training) 
  }else{
    print("Training dataframe created")
    testing <- Data[-training_index,]
    return (testing)
  }
}

#Correlation Function
cor_fun<- function(gene_exprs_DF,cor_cutoff=0.8) {
  # find highly correlated probes
  correlationMatrix <- cor(gene_exprs_DF)
  # find attributes that are highly corrected (ideally >0.75)
  findCorrelation(correlationMatrix, cutoff=cor_cutoff)
}


##### Clean Training DF function. Correlation and variance.  
f_train_clean <- function (train_df, cor_cutoff=0.75,low_variance_cut=0.1){
  ### Remove High correlation probes. 
  #make dataset of features that are not probes for backup and later use.
  highlyCorrelated<-cor_fun(train_df[,-c(1)],cor_cutoff=cor_cutoff)
  # remove highly correlated probes
  train_df<-train_df[,-c(highlyCorrelated+1)]##+1 is for phenotype col. Adds 1 to each number. 
  
  #Low Variance Probes removed
  Gx_vars<-apply(train_df[,c(-1)], 2, function(x) var(x,y = NULL, na.rm = FALSE, "everything"))
  Gx_vars_quant<-quantile(Gx_vars, c(low_variance_cut))
  Gx_vars_names<-names(which(Gx_vars > Gx_vars_quant[1]))
  train_df<-train_df[,c("Phenotype",Gx_vars_names)]
  return(train_df)
}


##### Variables
Number_of_Splits <- 10 #Number of splits
Split_Percentage <- 0.70 #Split of data
GX_DF <- Gx_DF_1_locHS #Expression data with Phenotype as first column

#for data partition
set.seed(Number_of_Splits)
DataPartition_DF <- createDataPartition(GX_DF$Phenotype,times = Number_of_Splits,  p = Split_Percentage, list = F)


#Create Training DF
training_df<-f_train_test(GX_DF,DataPartition_DF,1,T)
#Create Clean DF
training_df_small<-f_train_clean(training_df,cor_cutoff = 0.75)

dim(training_df_small)


```

# Step 2: Feature Selection 
```{r Feature Selection}

Filter methods
Removal of  highly correlated probes. 80%
Low variation probes. 10% percent removal.
Diff Expression list 
Wrapper methods
Recursive feature elimination. 5x3

subsets <- c(50,100,150,200,250,300)


#Allow 8 Cores
registerDoMC(cores = 8) 
#Set Seed
seed = 7

#set seed
set.seed(seed)
#set control parameters 
control <- rfeControl(functions=rfFuncs, #Random Forrest based
                      method="repeatedcv", #Repeated CV
                      number=5, #10 folds
                      repeats=3, #10 repeats
                      verbose = FALSE)

# run the RFE algorithm
rfe_output <- rfe(gx_predictors, class_labels, sizes=subsets, rfeControl=control)
rfe_output_prepro<-rfe_output
rfe_output <- rfe(x=training_cor_df[,-1], y=class_labels, sizes=subsets, rfeControl=control)
#Save output files
save(rfe_output, rfe_output_prepro, file=paste(P1_output_dir,"RFE_model_results_hypothesis_free_data.Rdata",sep=""), compress = T)
load(paste(P1_output_dir,"RFE_model_results_hypothesis_free_data.Rdata",sep=""))




```

# Step 3: Build Models 
```{r Build Models}

Algorithms. 
I have a decent list of algorithms that I am using. This is useful for ensemble aswell. Not changing anything here.5x3 - 10x10 fold cross validation. 
Tuning.
I have the scripts for this, and am basically just expanding parameter search. 
Massivivly increasing grid search. 3-20 per parameter?
Ensembles.
Find models that are uncorrelated in predictions but accurate.
Linear model to combine maximum 3 models. 

```


# Step 4: Outputs
```{r Outputs}

Output:
Plots of Models
Performance
Tables for latex


```